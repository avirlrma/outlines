{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Generate text with LLMs","text":"<p> Robust prompting &amp; (guided) text generation  Get started  Contribute <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr\n\nimport outlines.models as models\nimport outlines.text.generate as generate\n\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: int\n    armor: Armor\n    strength: int\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.json(model, Character, max_tokens=100)\nsequence = generator(\"Give me a character description\")\n</code></pre> <p></p>"},{"location":"feedback/","title":"Feedback","text":"<p>We highly value the insights of our users, and we would love to hear from you. If you are using Outlines for your projects and would like to share your experience with us, let's connect:</p> <ul> <li>What are you building with it?</li> <li>What do you like about it?</li> <li>What challenges are you facing?</li> <li>What do you think could be improved?</li> </ul> <p>To schedule an appointment follow this link. This is exclusively intended to share your experience, please go on Discord or GitHub for support.</p>"},{"location":"get_started/","title":"Getting started","text":""},{"location":"get_started/#1-installation","title":"1. Installation","text":"<p>Outlines is available on PyPi:</p> <pre><code>pip install outlines\n</code></pre> <p>Model integrations</p> <p>The following model integrations are available. To use them you must install the required dependencies:</p> <ul> <li><code>openai</code> for OpenAI models;</li> <li><code>transformers</code> for Hugging Face models;</li> <li><code>autoawq</code> for AWQ models;</li> <li><code>auto-gptq</code> for GPTQ models;</li> <li><code>mamba_ssm</code> for Mamba models.</li> </ul>"},{"location":"get_started/#2-hello-world","title":"2. Hello, World","text":"<p>A very simple Outlines program looks like:</p> CodeOutput <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"gpt2\")\ngenerator = outlines.generate.format(model, int)\n\ngenerate(\"2+2=\")\n</code></pre> <pre><code>4\n</code></pre> <p>The program goes through the following steps:</p> <ol> <li>Initialize the model using the <code>transformers</code> library. Weights are loaded in memory;</li> <li>Initialize the generator. <code>outlines.generate.format</code> constraints the output of the model    to be a valid Python data type.</li> <li>Call the generator with a prompt.</li> </ol>"},{"location":"get_started/#3-going-further","title":"3. Going further","text":"<p>If you need more inspiration you can take a look at the Examples. If you have any question, or requests for documentation please reach out to us on GitHub, Twitter or Discord.</p>"},{"location":"get_started/#4-acknowledgements","title":"4. Acknowledgements","text":"<p>Outlines was originally developed at @NormalComputing by @remilouf and @BrandonTWillard.</p>"},{"location":"licence/","title":"Licence","text":"<p>Outlines is licenced under the Apache 2.0 licence. Please follow its terms when re-using part of Outlines' codebase.</p> <pre><code>                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023- The Outlines developers\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/fsm/","title":"Fsm","text":""},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM","title":"<code>RegexFSM</code>","text":"<p>             Bases: <code>FSM</code></p> <p>FSM to generate text that is in the language of a regular expression.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>class RegexFSM(FSM):\n    \"\"\"FSM to generate text that is in the language of a regular expression.\"\"\"\n\n    def __init__(\n        self,\n        regex_string: str,\n        tokenizer: \"Tokenizer\",\n        max_tokens: Optional[int] = None,\n    ):\n        regex_pattern = interegular.parse_pattern(regex_string)\n        regex_fsm, _ = make_deterministic_fsm(regex_pattern.to_fsm().reduce())\n        (\n            self.states_to_token_maps,\n            self.empty_token_ids,\n        ) = create_fsm_index_tokenizer(regex_fsm, tokenizer)\n\n        # We make sure that it is possible to generate strings in the language\n        # of the regular expression with the tokens present in the model's\n        # vocabulary.\n        if not any(\n            regex_fsm.finals.intersection(v.values())\n            for v in self.states_to_token_maps.values()\n        ):\n            raise ValueError(\n                \"The vocabulary does not allow us to build a sequence that matches the input regex\"\n            )\n\n        self.final_states = regex_fsm.finals | {\n            -1\n        }  # Include the EOS token in final states\n        self.max_tokens = max_tokens\n        self.num_tokens_generated = 0\n        self.vocabulary = tokenizer.vocabulary.values()\n        self.end_token_id = tokenizer.eos_token_id\n\n    def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n        \"\"\"Generate a list of allowed tokens for the next step.\n\n        The initialization of the FSM builds an index which maps FSM states to a\n        map from authorized tokens to the state in which the FSM needs to move\n        if said token is generated. Therefore the authorized tokens at the\n        current state are the keys of the map returned by the value of the index\n        for current state.\n\n        If the current state is not contained in the end this means that we are\n        in a final state of the FSM. We only authorize EOS tokens in the final\n        state.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM\n\n        Returns\n        -------\n        A list that contains the tokens to mask.\n\n        \"\"\"\n        next_tokens_to_end_states = self.states_to_token_maps.get(state)\n\n        if next_tokens_to_end_states is None:\n            return [self.end_token_id]\n        else:\n            return list(next_tokens_to_end_states.keys())\n\n    def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n        \"\"\"Update the state of the FSM.\n\n        We use the index to determine to which state the FSM should transition\n        given the token that was just generated.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The new state of the FSM.\n\n        \"\"\"\n        self.num_tokens_generated += 1\n\n        if self.max_tokens is not None:\n            if self.num_tokens_generated == self.max_tokens:\n                return FSMState(-1)\n\n        if token_id == self.end_token_id:\n            return FSMState(-1)\n\n        last_token_to_end_state = self.states_to_token_maps[state]\n        next_state = last_token_to_end_state.get(token_id)\n        if next_state is None:\n            next_state = -1\n\n        return FSMState(next_state)\n\n    def is_final_state(self, state: FSMState) -&gt; bool:\n        \"\"\"Determine whether the current state of the FSM is a final state.\"\"\"\n        return state in self.final_states\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.allowed_token_ids","title":"<code>allowed_token_ids(state)</code>","text":"<p>Generate a list of allowed tokens for the next step.</p> <p>The initialization of the FSM builds an index which maps FSM states to a map from authorized tokens to the state in which the FSM needs to move if said token is generated. Therefore the authorized tokens at the current state are the keys of the map returned by the value of the index for current state.</p> <p>If the current state is not contained in the end this means that we are in a final state of the FSM. We only authorize EOS tokens in the final state.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.allowed_token_ids--parameters","title":"Parameters","text":"<p>state     The current state of the FSM</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.allowed_token_ids--returns","title":"Returns","text":"<p>A list that contains the tokens to mask.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n    \"\"\"Generate a list of allowed tokens for the next step.\n\n    The initialization of the FSM builds an index which maps FSM states to a\n    map from authorized tokens to the state in which the FSM needs to move\n    if said token is generated. Therefore the authorized tokens at the\n    current state are the keys of the map returned by the value of the index\n    for current state.\n\n    If the current state is not contained in the end this means that we are\n    in a final state of the FSM. We only authorize EOS tokens in the final\n    state.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM\n\n    Returns\n    -------\n    A list that contains the tokens to mask.\n\n    \"\"\"\n    next_tokens_to_end_states = self.states_to_token_maps.get(state)\n\n    if next_tokens_to_end_states is None:\n        return [self.end_token_id]\n    else:\n        return list(next_tokens_to_end_states.keys())\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.is_final_state","title":"<code>is_final_state(state)</code>","text":"<p>Determine whether the current state of the FSM is a final state.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def is_final_state(self, state: FSMState) -&gt; bool:\n    \"\"\"Determine whether the current state of the FSM is a final state.\"\"\"\n    return state in self.final_states\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.next_state","title":"<code>next_state(state, token_id)</code>","text":"<p>Update the state of the FSM.</p> <p>We use the index to determine to which state the FSM should transition given the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.next_state--parameters","title":"Parameters","text":"<p>state     The current state of the FSM. token_id     The id of the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.next_state--returns","title":"Returns","text":"<p>The new state of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n    \"\"\"Update the state of the FSM.\n\n    We use the index to determine to which state the FSM should transition\n    given the token that was just generated.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The new state of the FSM.\n\n    \"\"\"\n    self.num_tokens_generated += 1\n\n    if self.max_tokens is not None:\n        if self.num_tokens_generated == self.max_tokens:\n            return FSMState(-1)\n\n    if token_id == self.end_token_id:\n        return FSMState(-1)\n\n    last_token_to_end_state = self.states_to_token_maps[state]\n    next_state = last_token_to_end_state.get(token_id)\n    if next_state is None:\n        next_state = -1\n\n    return FSMState(next_state)\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM","title":"<code>StopAtTokenFSM</code>","text":"<p>             Bases: <code>FSM</code></p> <p>FSM to generate text until a specified token id is generated or a specified number of tokens has been generated.</p> <p>Text is usually produced until the EOS token is generated by the model.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>class StopAtTokenFSM(FSM):\n    \"\"\"FSM to generate text until a specified token id is generated or\n    a specified number of tokens has been generated.\n\n    Text is usually produced until the EOS token is generated by the\n    model.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer: \"Tokenizer\",\n        stop_token_id: int,\n        max_tokens: Optional[int] = None,\n    ):\n        self.stop_token_id = stop_token_id\n        self.max_tokens = max_tokens\n        self.num_tokens_generated = 0\n        self.vocabulary = tokenizer.vocabulary.values()\n        self.final_states = {1}\n\n    def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n        \"\"\"Generate a list of allowed tokens for the next step.\n\n        When in the initial state we allow every token to be generated.\n        In the final state the only allowed token is `stop_token_id`.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM\n\n        Returns\n        -------\n        A list that contains the tokens to mask.\n\n        \"\"\"\n        if state == 0:\n            return list(self.vocabulary)\n        else:\n            return [self.stop_token_id]\n\n    def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n        \"\"\"Update the state of the FSM.\n\n        The FSM stays in the initial state `0` unless the specified stop token\n        has been generated or the maximum number of tokens has been reached. In\n        which case the FSM moves to the final state `1`.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The new state of the FSM.\n\n        \"\"\"\n        self.num_tokens_generated += 1\n\n        if self.max_tokens is not None:\n            if self.num_tokens_generated &gt;= self.max_tokens:\n                return FSMState(1)\n\n        if token_id == self.stop_token_id:\n            return FSMState(1)\n\n        return FSMState(0)\n\n    def is_final_state(self, state: FSMState) -&gt; bool:\n        \"\"\"Determine whether the current state of the FSM is a final state.\"\"\"\n        return state in self.final_states\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.allowed_token_ids","title":"<code>allowed_token_ids(state)</code>","text":"<p>Generate a list of allowed tokens for the next step.</p> <p>When in the initial state we allow every token to be generated. In the final state the only allowed token is <code>stop_token_id</code>.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.allowed_token_ids--parameters","title":"Parameters","text":"<p>state     The current state of the FSM</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.allowed_token_ids--returns","title":"Returns","text":"<p>A list that contains the tokens to mask.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n    \"\"\"Generate a list of allowed tokens for the next step.\n\n    When in the initial state we allow every token to be generated.\n    In the final state the only allowed token is `stop_token_id`.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM\n\n    Returns\n    -------\n    A list that contains the tokens to mask.\n\n    \"\"\"\n    if state == 0:\n        return list(self.vocabulary)\n    else:\n        return [self.stop_token_id]\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.is_final_state","title":"<code>is_final_state(state)</code>","text":"<p>Determine whether the current state of the FSM is a final state.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def is_final_state(self, state: FSMState) -&gt; bool:\n    \"\"\"Determine whether the current state of the FSM is a final state.\"\"\"\n    return state in self.final_states\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.next_state","title":"<code>next_state(state, token_id)</code>","text":"<p>Update the state of the FSM.</p> <p>The FSM stays in the initial state <code>0</code> unless the specified stop token has been generated or the maximum number of tokens has been reached. In which case the FSM moves to the final state <code>1</code>.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.next_state--parameters","title":"Parameters","text":"<p>state     The current state of the FSM. token_id     The id of the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtTokenFSM.next_state--returns","title":"Returns","text":"<p>The new state of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n    \"\"\"Update the state of the FSM.\n\n    The FSM stays in the initial state `0` unless the specified stop token\n    has been generated or the maximum number of tokens has been reached. In\n    which case the FSM moves to the final state `1`.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The new state of the FSM.\n\n    \"\"\"\n    self.num_tokens_generated += 1\n\n    if self.max_tokens is not None:\n        if self.num_tokens_generated &gt;= self.max_tokens:\n            return FSMState(1)\n\n    if token_id == self.stop_token_id:\n        return FSMState(1)\n\n    return FSMState(0)\n</code></pre>"},{"location":"api/json_schema/","title":"Json schema","text":""},{"location":"api/json_schema/#outlines.fsm.json_schema.build_regex_from_object","title":"<code>build_regex_from_object(object)</code>","text":"<p>Turn a JSON schema into a regex that matches any JSON object that follows this schema.</p> <p>JSON Schema is a declarative language that allows to annotate JSON documents with types and descriptions. These schemas can be generated from any Python datastructure that has type annotation: namedtuples, dataclasses, Pydantic models. And by ensuring that the generation respects the schema we ensure that the output can be parsed into these objects. This function parses the provided schema and builds a generation schedule which mixes deterministic generation (fixed strings), and sampling with constraints.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.build_regex_from_object--parameters","title":"Parameters","text":"<p>schema     A string that represents a JSON Schema.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.build_regex_from_object--returns","title":"Returns","text":"<p>A generation schedule. A list of strings that represent the JSON schema's structure and regular expression that define the structure of the fields.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.build_regex_from_object--references","title":"References","text":"<p>.. [0] JSON Schema. https://json-schema.org/</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def build_regex_from_object(object: Union[str, Callable, BaseModel]):\n    \"\"\"Turn a JSON schema into a regex that matches any JSON object that follows\n    this schema.\n\n    JSON Schema is a declarative language that allows to annotate JSON documents\n    with types and descriptions. These schemas can be generated from any Python\n    datastructure that has type annotation: namedtuples, dataclasses, Pydantic\n    models. And by ensuring that the generation respects the schema we ensure\n    that the output can be parsed into these objects.\n    This function parses the provided schema and builds a generation schedule which\n    mixes deterministic generation (fixed strings), and sampling with constraints.\n\n    Parameters\n    ----------\n    schema\n        A string that represents a JSON Schema.\n\n    Returns\n    -------\n    A generation schedule. A list of strings that represent the JSON\n    schema's structure and regular expression that define the structure of\n    the fields.\n\n    References\n    ----------\n    .. [0] JSON Schema. https://json-schema.org/\n\n    \"\"\"\n\n    if isinstance(object, type(BaseModel)):\n        schema = object.model_json_schema()\n    elif callable(object):\n        schema = get_schema_from_signature(object)\n    else:\n        schema = json.loads(object)\n\n    Validator.check_schema(schema)\n\n    # Build reference resolver\n    schema = Resource(contents=schema, specification=DRAFT202012)\n    uri = schema.id() if schema.id() is not None else \"\"\n    registry = Registry().with_resource(uri=uri, resource=schema)\n    resolver = registry.resolver()\n\n    content = schema.contents\n    return to_regex(resolver, content)\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; str:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    model = create_model(\"Arguments\", **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.to_regex","title":"<code>to_regex(resolver, instance)</code>","text":"<p>Translate a JSON Schema instance into a regex that validates the schema.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.to_regex--note","title":"Note","text":"<p>Many features of JSON schema are missing: - Support the fact that fields in an object are optional by default - Handle <code>required</code> keyword - Handle <code>additionalProperties</code> keyword - Handle types defined as a list - Handle constraints on numbers - Handle special patterns: <code>date</code>, <code>uri</code>, etc. - Handle optional fields (not in <code>required</code>)</p> <p>This does not support recursive definitions.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.to_regex--parameters","title":"Parameters","text":"<p>resolver     An object that resolves references to other instances within a schema instance     The instance to translate</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def to_regex(resolver: Resolver, instance: dict):\n    \"\"\"Translate a JSON Schema instance into a regex that validates the schema.\n\n    Note\n    ----\n    Many features of JSON schema are missing:\n    - Support the fact that fields in an object are optional by default\n    - Handle `required` keyword\n    - Handle `additionalProperties` keyword\n    - Handle types defined as a list\n    - Handle constraints on numbers\n    - Handle special patterns: `date`, `uri`, etc.\n    - Handle optional fields (not in `required`)\n\n    This does not support recursive definitions.\n\n    Parameters\n    ----------\n    resolver\n        An object that resolves references to other instances within a schema\n    instance\n        The instance to translate\n    \"\"\"\n    whitespace = r\"[\\n ]*\"\n\n    if \"properties\" in instance:\n        regex = \"\"\n        regex += r\"\\{\"\n        for i, (name, value) in enumerate(instance[\"properties\"].items()):\n            regex += f'{whitespace}\"{name}\"{whitespace}:{whitespace}'\n            regex += to_regex(resolver, value)\n\n            # No comma after the last key-value pair in JSON\n            if i &lt; len(instance[\"properties\"]) - 1:\n                regex += f\"{whitespace},\"\n\n        regex += f\"{whitespace}\" + r\"\\}\"\n\n        return regex\n\n    # To validate against allOf, the given data must be valid against all of the\n    # given subschemas.\n    elif \"allOf\" in instance:\n        subregexes = [to_regex(resolver, t) for t in instance[\"allOf\"]]\n        subregexes_str = [f\"{subregex}\" for subregex in subregexes]\n        return rf\"({''.join(subregexes_str)})\"\n\n    # To validate against `anyOf`, the given data must be valid against\n    # any (one or more) of the given subschemas.\n    elif \"anyOf\" in instance:\n        subregexes = [to_regex(resolver, t) for t in instance[\"anyOf\"]]\n        combinations = [\n            \"(\" + \"\".join(c) + \")\"\n            for r in range(1, len(subregexes) + 1)\n            for c in it.permutations(subregexes, r)\n        ]\n\n        return rf\"({'|'.join(combinations)})\"\n\n    # To validate against oneOf, the given data must be valid against exactly\n    # one of the given subschemas.\n    elif \"oneOf\" in instance:\n        subregexes = [to_regex(resolver, t) for t in instance[\"oneOf\"]]\n\n        xor_patterns = []\n        # json schema validation ensured there is no overlapping schemas in oneOf\n        for subregex in subregexes:\n            other_subregexes = filter(lambda r: r != subregex, subregexes)\n            other_subregexes_str = \"|\".join([f\"{s}\" for s in other_subregexes])\n            negative_lookahead = f\"(?!.*({other_subregexes_str}))\"\n            xor_patterns.append(f\"({subregex}){negative_lookahead}\")\n\n        return rf\"({'|'.join(xor_patterns)})\"\n\n    # The enum keyword is used to restrict a value to a fixed set of values. It\n    # must be an array with at least one element, where each element is unique.\n    elif \"enum\" in instance:\n        choices = []\n        for choice in instance[\"enum\"]:\n            if type(choice) in [int, float, bool, None]:\n                choices.append(re.escape(str(choice)))\n            elif type(choice) == str:\n                choices.append(f'\"{re.escape(choice)}\"')\n\n        return f\"({'|'.join(choices)})\"\n\n    elif \"$ref\" in instance:\n        path = f\"{instance['$ref']}\"\n        instance = resolver.lookup(path).contents\n        return to_regex(resolver, instance)\n\n    # The type keyword may either be a string or an array:\n    # - If it's a string, it is the name of one of the basic types.\n    # - If it is an array, it must be an array of strings, where each string is\n    # the name of one of the basic types, and each element is unique. In this\n    # case, the JSON snippet is valid if it matches any of the given types.\n    elif \"type\" in instance:\n        instance_type = instance[\"type\"]\n        if instance_type == \"string\":\n            if \"maxLength\" in instance or \"minLength\" in instance:\n                max_items = instance.get(\"maxLength\", \"\")\n                min_items = instance.get(\"minLength\", \"\")\n                try:\n                    if int(max_items) &lt; int(min_items):\n                        raise ValueError(\n                            \"maxLength must be greater than or equal to minLength\"\n                        )\n                except ValueError:\n                    pass\n                return f'\"{STRING_INNER}{{{min_items},{max_items}}}\"'\n            elif \"pattern\" in instance:\n                pattern = instance[\"pattern\"]\n                if pattern[0] == \"^\" and pattern[-1] == \"$\":\n                    return rf'(^\"{pattern[1:-1]}\"$)'\n                else:\n                    return rf'(\"{pattern}\")'\n            else:\n                return type_to_regex[\"string\"]\n\n        elif instance_type == \"number\":\n            return type_to_regex[\"number\"]\n\n        elif instance_type == \"integer\":\n            return type_to_regex[\"integer\"]\n\n        elif instance_type == \"array\":\n            min_items = instance.get(\"minItems\", \"0\")\n            max_items = instance.get(\"maxItems\", \"\")\n            if min_items == max_items:\n                num_repeats = \"{\" + str(int(min_items) - 1) + \"}\"\n            else:\n                num_repeats = \"*\"\n\n            if \"items\" in instance:\n                items_regex = to_regex(resolver, instance[\"items\"])\n                return rf\"\\[({items_regex})(,({items_regex})){num_repeats}\\]\"\n            else:\n                # Here we need to make the choice to exclude generating list of objects\n                # if the specification of the object is not given, even though a JSON\n                # object that contains an object here would be valid under the specification.\n                types = [\n                    {\"type\": \"boolean\"},\n                    {\"type\": \"null\"},\n                    {\"type\": \"number\"},\n                    {\"type\": \"integer\"},\n                    {\"type\": \"string\"},\n                ]\n                regexes = [to_regex(resolver, t) for t in types]\n                return (\n                    rf\"\\[({'|'.join(regexes)})(,({'|'.join(regexes)})){num_repeats}\\]\"\n                )\n\n        elif instance_type == \"boolean\":\n            return type_to_regex[\"boolean\"]\n\n        elif instance_type == \"null\":\n            return type_to_regex[\"null\"]\n\n        elif isinstance(instance_type, list):\n            # Here we need to make the choice to exclude generating an object\n            # if the specification of the object is not give, even though a JSON\n            # object that contains an object here would be valid under the specification.\n            regexes = [\n                to_regex(resolver, {\"type\": t}) for t in instance_type if t != \"object\"\n            ]\n            return rf\"({'|'.join(regexes)})\"\n\n    raise NotImplementedError(\n        f\"\"\"Could not translate the instance {instance} to a\n    regular expression. Make sure it is valid to the JSON Schema specification. If\n    it is, please open an issue on the Outlines repository\"\"\"\n    )\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer","title":"<code>Transformer</code>","text":"<p>Represents a <code>transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformer:\n    \"\"\"Represents a `transformers` model.\"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        self.device = model.device\n        self.model = model\n        self.tokenizer = tokenizer\n\n    @torch.inference_mode\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: torch.LongTensor,\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; Tuple[torch.FloatTensor, Optional[KVCacheType]]:\n        \"\"\"Compute a forward pass through the transformer model.\n\n        Parameters\n        ----------\n        input_ids\n            The input token ids.  Must be one or two dimensional.\n        attention_mask\n            The attention mask.  Must be one or two dimensional.\n        past_key_values\n            A tuple of tuples containing the cached key and value tensors for each\n            attention head.\n\n        Returns\n        -------\n        The computed logits and the new cached key and value tensors.\n\n        \"\"\"\n        assert 0 &lt; input_ids.ndim &lt; 3\n\n        if past_key_values:\n            input_ids = input_ids[..., -1].unsqueeze(-1)\n\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            output_attentions=False,\n            output_hidden_states=False,\n            past_key_values=past_key_values,\n        )\n\n        return output.logits, output.past_key_values\n\n    def __call__(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: torch.LongTensor,\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; torch.FloatTensor:\n        logits, kv_cache = self.forward(input_ids, attention_mask, past_key_values)\n        next_token_logits = logits[..., -1, :]\n\n        return next_token_logits, kv_cache\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward","title":"<code>forward(input_ids, attention_mask, past_key_values=None)</code>","text":"<p>Compute a forward pass through the transformer model.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward--parameters","title":"Parameters","text":"<p>input_ids     The input token ids.  Must be one or two dimensional. attention_mask     The attention mask.  Must be one or two dimensional. past_key_values     A tuple of tuples containing the cached key and value tensors for each     attention head.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward--returns","title":"Returns","text":"<p>The computed logits and the new cached key and value tensors.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@torch.inference_mode\ndef forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: torch.LongTensor,\n    past_key_values: Optional[Tuple] = None,\n) -&gt; Tuple[torch.FloatTensor, Optional[KVCacheType]]:\n    \"\"\"Compute a forward pass through the transformer model.\n\n    Parameters\n    ----------\n    input_ids\n        The input token ids.  Must be one or two dimensional.\n    attention_mask\n        The attention mask.  Must be one or two dimensional.\n    past_key_values\n        A tuple of tuples containing the cached key and value tensors for each\n        attention head.\n\n    Returns\n    -------\n    The computed logits and the new cached key and value tensors.\n\n    \"\"\"\n    assert 0 &lt; input_ids.ndim &lt; 3\n\n    if past_key_values:\n        input_ids = input_ids[..., -1].unsqueeze(-1)\n\n    output = self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        return_dict=True,\n        output_attentions=False,\n        output_hidden_states=False,\n        past_key_values=past_key_values,\n    )\n\n    return output.logits, output.past_key_values\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>             Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, model_name: str, **kwargs):\n        from transformers import AutoTokenizer\n\n        kwargs.setdefault(\"padding_side\", \"left\")\n        self.model_name = model_name\n        # TODO: Do something to make this hashable?\n        self.kwargs = kwargs\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **kwargs)\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n\n        if not self.tokenizer.pad_token_id:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[torch.LongTensor, torch.LongTensor]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: torch.LongTensor) -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            return other.model_name == self.model_name and other.kwargs == self.kwargs\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.transformers","title":"<code>transformers(model_name, device=None, model_kwargs={}, tokenizer_kwargs={})</code>","text":"<p>Instantiate a model from the <code>transformers</code> library and its tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--parameters","title":"Parameters","text":"<p>model_name     The name of the model as listed on Hugging Face's model page. device     The device(s) on which the model should be loaded. This overrides     the <code>device_map</code> entry in <code>model_kwargs</code> when provided. model_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the model. tokenizer_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--returns","title":"Returns","text":"<p>A <code>TransformersModel</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def transformers(\n    model_name: str,\n    device: Optional[str] = None,\n    model_kwargs: dict = {},\n    tokenizer_kwargs: dict = {},\n):\n    \"\"\"Instantiate a model from the `transformers` library and its tokenizer.\n\n    Parameters\n    ----------\n    model_name\n        The name of the model as listed on Hugging Face's model page.\n    device\n        The device(s) on which the model should be loaded. This overrides\n        the `device_map` entry in `model_kwargs` when provided.\n    model_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the model.\n    tokenizer_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the tokenizer.\n\n    Returns\n    -------\n    A `TransformersModel` model instance.\n\n    \"\"\"\n    try:\n        from transformers import AutoModelForCausalLM\n    except ImportError:\n        raise ImportError(\n            \"The `transformers` library needs to be installed in order to use `transformers` models.\"\n        )\n\n    if device is not None:\n        model_kwargs[\"device_map\"] = device\n\n    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n    tokenizer = TransformerTokenizer(model_name, **tokenizer_kwargs)\n\n    return Transformer(model, tokenizer)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>An object that represents the OpenAI API.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI:\n    \"\"\"An object that represents the OpenAI API.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        api_key: Optional[str] = None,\n        max_retries: int = 6,\n        timeout: Optional[float] = None,\n        system_prompt: Optional[str] = None,\n        config: Optional[OpenAIConfig] = None,\n    ):\n        \"\"\"Create an `OpenAI` instance.\n\n        Parameters\n        ----------\n        model_name\n            Model to use, as defined in OpenAI's documentation\n        api_key\n            Secret key to use with the OpenAI API. One can also set the\n            `OPENAI_API_KEY` environment variable, or the value of\n            `openai.api_key`.\n        max_retries\n            The maximum number of retries when calls to the API fail.\n        timeout\n            Duration after which the request times out.\n        system_prompt\n            The content of the system message that precedes the user's prompt.\n        config\n            An instance of `OpenAIConfig`. Can be useful to specify some\n            parameters that cannot be set by calling this class' methods.\n\n        \"\"\"\n        try:\n            import openai\n        except ImportError:\n            raise ImportError(\n                \"The `openai` library needs to be installed in order to use Outlines' OpenAI integration.\"\n            )\n\n        if api_key is None:\n            if os.getenv(\"OPENAI_API_KEY\") is not None:\n                api_key = os.getenv(\"OPENAI_API_KEY\")\n            elif openai.api_key is not None:\n                api_key = openai.api_key\n            else:\n                raise ValueError(\n                    \"You must specify an API key to use the OpenAI API integration.\"\n                )\n\n        if config is not None:\n            self.config = replace(config, model=model_name)  # type: ignore\n        else:\n            self.config = OpenAIConfig(model=model_name)\n\n        self.client = openai.AsyncOpenAI(\n            api_key=api_key, max_retries=max_retries, timeout=timeout\n        )\n        self.system_prompt = system_prompt\n\n        # We count the total number of prompt and generated tokens as returned\n        # by the OpenAI API, summed over all the requests performed with this\n        # model instance.\n        self.prompt_tokens = 0\n        self.completion_tokens = 0\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        max_tokens: Optional[int] = None,\n        *,\n        temperature: float = 1.0,\n        samples: int = 1,\n        stop_at: Optional[Union[List[str], str]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Call the OpenAI API to generate text.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        max_tokens\n            The maximum number of tokens to generate\n        temperature\n            The value of the temperature used to sample tokens\n        samples\n            The number of completions to generate for each prompt\n        stop_at\n            Up to 4 words where the API will stop the completion.\n\n        \"\"\"\n        config = replace(self.config, max_tokens=max_tokens, n=samples, stop=stop_at)  # type: ignore\n\n        if \"text-\" in self.config.model:\n            raise NotImplementedError(\n                textwrap.dedent(\n                    \"Most models that support the legacy completion endpoints will be \"\n                    \"deprecated on January 2024. Use Chat models instead.\\n\"\n                    \"The list of chat models is available at https://platform.openai.com/docs/guides/text-generation.\"\n                )\n            )\n        if \"gpt-\" in self.config.model:\n            response, usage = generate_chat(\n                prompt, self.system_prompt, self.client, config\n            )\n            self.prompt_tokens += usage[\"prompt_tokens\"]\n            self.completion_tokens += usage[\"completion_tokens\"]\n\n            return response\n\n    def generate_choice(\n        self, prompt: str, choices: List[str], max_tokens: Optional[int] = None\n    ) -&gt; str:\n        \"\"\"Call the OpenAI API to generate one of several choices.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        choices\n            The list of strings between which we ask the model to choose\n        max_tokens\n            The maximum number of tokens to generate\n\n        \"\"\"\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"The `tiktoken` library needs to be installed in order to choose `outlines.models.openai` with `is_in`\"\n            )\n\n        config = replace(self.config, max_tokens=max_tokens)\n\n        tokenizer = tiktoken.encoding_for_model(self.config.model)\n\n        greedy = False\n        decoded: List[str] = []\n        encoded_choices_left: List[List[int]] = [\n            tokenizer.encode(word) for word in choices\n        ]\n\n        while len(encoded_choices_left) &gt; 0:\n            max_tokens_left = max([len(tokens) for tokens in encoded_choices_left])\n            transposed_choices_left: List[Set] = [\n                {item for item in subset if item is not None}\n                for subset in zip_longest(*encoded_choices_left)\n            ]\n\n            if not greedy:\n                mask = build_optimistic_mask(transposed_choices_left)\n            else:\n                mask = {}\n                for token in transposed_choices_left[0]:  # build greedy mask\n                    mask[token] = 100\n\n            if len(mask) == 0:\n                break\n\n            config = replace(config, logit_bias=mask, max_tokens=max_tokens_left)\n\n            response, usage = generate_chat(\n                prompt, self.system_prompt, self.client, config\n            )\n            self.completion_tokens += usage[\"completion_tokens\"]\n            self.prompt_tokens += usage[\"prompt_tokens\"]\n\n            encoded_response = tokenizer.encode(response)\n\n            if encoded_response in encoded_choices_left:\n                decoded.append(response)\n                break\n            else:\n                (\n                    encoded_response,\n                    encoded_choices_left,\n                ) = find_response_choices_intersection(\n                    encoded_response, encoded_choices_left\n                )\n\n                if len(encoded_response) == 0:\n                    greedy = True  # next iteration will be \"greedy\"\n                    continue\n                else:\n                    decoded.append(\"\".join(tokenizer.decode(encoded_response)))\n\n                    if len(encoded_choices_left) == 1:  # only one choice left\n                        choice_left = tokenizer.decode(encoded_choices_left[0])\n                        decoded.append(choice_left)\n                        break\n\n                    greedy = False  # after each success, stay with (or switch to) \"optimistic\" approach\n\n                prompt = prompt + \"\".join(decoded)\n\n        choice = \"\".join(decoded)\n\n        return choice\n\n    def generate_json(self):\n        \"\"\"Call the OpenAI API to generate a JSON object.\"\"\"\n        raise NotImplementedError\n\n    def __str__(self):\n        return self.__class__.__name__ + \" API\"\n\n    def __repr__(self):\n        return str(self.config)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__","title":"<code>__call__(prompt, max_tokens=None, *, temperature=1.0, samples=1, stop_at=None)</code>","text":"<p>Call the OpenAI API to generate text.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model max_tokens     The maximum number of tokens to generate temperature     The value of the temperature used to sample tokens samples     The number of completions to generate for each prompt stop_at     Up to 4 words where the API will stop the completion.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __call__(\n    self,\n    prompt: Union[str, List[str]],\n    max_tokens: Optional[int] = None,\n    *,\n    temperature: float = 1.0,\n    samples: int = 1,\n    stop_at: Optional[Union[List[str], str]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Call the OpenAI API to generate text.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    max_tokens\n        The maximum number of tokens to generate\n    temperature\n        The value of the temperature used to sample tokens\n    samples\n        The number of completions to generate for each prompt\n    stop_at\n        Up to 4 words where the API will stop the completion.\n\n    \"\"\"\n    config = replace(self.config, max_tokens=max_tokens, n=samples, stop=stop_at)  # type: ignore\n\n    if \"text-\" in self.config.model:\n        raise NotImplementedError(\n            textwrap.dedent(\n                \"Most models that support the legacy completion endpoints will be \"\n                \"deprecated on January 2024. Use Chat models instead.\\n\"\n                \"The list of chat models is available at https://platform.openai.com/docs/guides/text-generation.\"\n            )\n        )\n    if \"gpt-\" in self.config.model:\n        response, usage = generate_chat(\n            prompt, self.system_prompt, self.client, config\n        )\n        self.prompt_tokens += usage[\"prompt_tokens\"]\n        self.completion_tokens += usage[\"completion_tokens\"]\n\n        return response\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(model_name, api_key=None, max_retries=6, timeout=None, system_prompt=None, config=None)</code>","text":"<p>Create an <code>OpenAI</code> instance.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__--parameters","title":"Parameters","text":"<p>model_name     Model to use, as defined in OpenAI's documentation api_key     Secret key to use with the OpenAI API. One can also set the     <code>OPENAI_API_KEY</code> environment variable, or the value of     <code>openai.api_key</code>. max_retries     The maximum number of retries when calls to the API fail. timeout     Duration after which the request times out. system_prompt     The content of the system message that precedes the user's prompt. config     An instance of <code>OpenAIConfig</code>. Can be useful to specify some     parameters that cannot be set by calling this class' methods.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    api_key: Optional[str] = None,\n    max_retries: int = 6,\n    timeout: Optional[float] = None,\n    system_prompt: Optional[str] = None,\n    config: Optional[OpenAIConfig] = None,\n):\n    \"\"\"Create an `OpenAI` instance.\n\n    Parameters\n    ----------\n    model_name\n        Model to use, as defined in OpenAI's documentation\n    api_key\n        Secret key to use with the OpenAI API. One can also set the\n        `OPENAI_API_KEY` environment variable, or the value of\n        `openai.api_key`.\n    max_retries\n        The maximum number of retries when calls to the API fail.\n    timeout\n        Duration after which the request times out.\n    system_prompt\n        The content of the system message that precedes the user's prompt.\n    config\n        An instance of `OpenAIConfig`. Can be useful to specify some\n        parameters that cannot be set by calling this class' methods.\n\n    \"\"\"\n    try:\n        import openai\n    except ImportError:\n        raise ImportError(\n            \"The `openai` library needs to be installed in order to use Outlines' OpenAI integration.\"\n        )\n\n    if api_key is None:\n        if os.getenv(\"OPENAI_API_KEY\") is not None:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n        elif openai.api_key is not None:\n            api_key = openai.api_key\n        else:\n            raise ValueError(\n                \"You must specify an API key to use the OpenAI API integration.\"\n            )\n\n    if config is not None:\n        self.config = replace(config, model=model_name)  # type: ignore\n    else:\n        self.config = OpenAIConfig(model=model_name)\n\n    self.client = openai.AsyncOpenAI(\n        api_key=api_key, max_retries=max_retries, timeout=timeout\n    )\n    self.system_prompt = system_prompt\n\n    # We count the total number of prompt and generated tokens as returned\n    # by the OpenAI API, summed over all the requests performed with this\n    # model instance.\n    self.prompt_tokens = 0\n    self.completion_tokens = 0\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_choice","title":"<code>generate_choice(prompt, choices, max_tokens=None)</code>","text":"<p>Call the OpenAI API to generate one of several choices.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_choice--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model choices     The list of strings between which we ask the model to choose max_tokens     The maximum number of tokens to generate</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_choice(\n    self, prompt: str, choices: List[str], max_tokens: Optional[int] = None\n) -&gt; str:\n    \"\"\"Call the OpenAI API to generate one of several choices.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    choices\n        The list of strings between which we ask the model to choose\n    max_tokens\n        The maximum number of tokens to generate\n\n    \"\"\"\n    try:\n        import tiktoken\n    except ImportError:\n        raise ImportError(\n            \"The `tiktoken` library needs to be installed in order to choose `outlines.models.openai` with `is_in`\"\n        )\n\n    config = replace(self.config, max_tokens=max_tokens)\n\n    tokenizer = tiktoken.encoding_for_model(self.config.model)\n\n    greedy = False\n    decoded: List[str] = []\n    encoded_choices_left: List[List[int]] = [\n        tokenizer.encode(word) for word in choices\n    ]\n\n    while len(encoded_choices_left) &gt; 0:\n        max_tokens_left = max([len(tokens) for tokens in encoded_choices_left])\n        transposed_choices_left: List[Set] = [\n            {item for item in subset if item is not None}\n            for subset in zip_longest(*encoded_choices_left)\n        ]\n\n        if not greedy:\n            mask = build_optimistic_mask(transposed_choices_left)\n        else:\n            mask = {}\n            for token in transposed_choices_left[0]:  # build greedy mask\n                mask[token] = 100\n\n        if len(mask) == 0:\n            break\n\n        config = replace(config, logit_bias=mask, max_tokens=max_tokens_left)\n\n        response, usage = generate_chat(\n            prompt, self.system_prompt, self.client, config\n        )\n        self.completion_tokens += usage[\"completion_tokens\"]\n        self.prompt_tokens += usage[\"prompt_tokens\"]\n\n        encoded_response = tokenizer.encode(response)\n\n        if encoded_response in encoded_choices_left:\n            decoded.append(response)\n            break\n        else:\n            (\n                encoded_response,\n                encoded_choices_left,\n            ) = find_response_choices_intersection(\n                encoded_response, encoded_choices_left\n            )\n\n            if len(encoded_response) == 0:\n                greedy = True  # next iteration will be \"greedy\"\n                continue\n            else:\n                decoded.append(\"\".join(tokenizer.decode(encoded_response)))\n\n                if len(encoded_choices_left) == 1:  # only one choice left\n                    choice_left = tokenizer.decode(encoded_choices_left[0])\n                    decoded.append(choice_left)\n                    break\n\n                greedy = False  # after each success, stay with (or switch to) \"optimistic\" approach\n\n            prompt = prompt + \"\".join(decoded)\n\n    choice = \"\".join(decoded)\n\n    return choice\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_json","title":"<code>generate_json()</code>","text":"<p>Call the OpenAI API to generate a JSON object.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_json(self):\n    \"\"\"Call the OpenAI API to generate a JSON object.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig","title":"<code>OpenAIConfig</code>  <code>dataclass</code>","text":"<p>Represents the parameters of the OpenAI API.</p> <p>The information was last fetched on 2023/11/20. We document below the properties that are specific to the OpenAI API. Not all these properties are supported by Outlines.</p>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig--properties","title":"Properties","text":"<p>model_name     The name of the model. Available models can be found on OpenAI's website. frequence_penalty     Number between 2.0 and -2.0. Positive values penalize new tokens based on     their existing frequency in the text, logit_bias     Modifies the likelihood of specified tokens to appear in the completion.     Number between -100 (forbid) and +100 (only allows). n     The number of completions to return for each prompt. presence_penalty     Similar to frequency penalty. response_format     Specifies the format the model must output. <code>{\"type\": \"json_object\"}</code>     enables JSON mode. seed     Two completions with the same <code>seed</code> value should return the same     completion. This is however not guaranteed. stop     Up to 4 words where the API will stop the completion. temperature     Number between 0 and 2. Higher values make the output more random, while     lower values make it more deterministic. top_p     Number between 0 and 1. Parameter for nucleus sampling. user     A unique identifier for the end-user.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@dataclass(frozen=True)\nclass OpenAIConfig:\n    \"\"\"Represents the parameters of the OpenAI API.\n\n    The information was last fetched on 2023/11/20. We document below the\n    properties that are specific to the OpenAI API. Not all these properties are\n    supported by Outlines.\n\n    Properties\n    ----------\n    model_name\n        The name of the model. Available models can be found on OpenAI's website.\n    frequence_penalty\n        Number between 2.0 and -2.0. Positive values penalize new tokens based on\n        their existing frequency in the text,\n    logit_bias\n        Modifies the likelihood of specified tokens to appear in the completion.\n        Number between -100 (forbid) and +100 (only allows).\n    n\n        The number of completions to return for each prompt.\n    presence_penalty\n        Similar to frequency penalty.\n    response_format\n        Specifies the format the model must output. `{\"type\": \"json_object\"}`\n        enables JSON mode.\n    seed\n        Two completions with the same `seed` value should return the same\n        completion. This is however not guaranteed.\n    stop\n        Up to 4 words where the API will stop the completion.\n    temperature\n        Number between 0 and 2. Higher values make the output more random, while\n        lower values make it more deterministic.\n    top_p\n        Number between 0 and 1. Parameter for nucleus sampling.\n    user\n        A unique identifier for the end-user.\n\n    \"\"\"\n\n    model: str = \"\"\n    frequency_penalty: float = 0\n    logit_bias: Dict[int, int] = field(default_factory=dict)\n    max_tokens: Optional[int] = None\n    n: int = 1\n    presence_penalty: float = 0\n    response_format: Optional[Dict[str, str]] = None\n    seed: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    temperature: Optional[float] = None\n    top_p: int = 1\n    user: str = field(default_factory=str)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.build_optimistic_mask","title":"<code>build_optimistic_mask(transposed, max_mask_size=300)</code>","text":"<p>We build the largest mask possible.</p> <p>Tokens are added from left to right, so if the encoded choices are e.g. <code>[[1,2], [3,4]]</code>, <code>1</code> and <code>3</code> will be added before <code>2</code> and <code>4</code>.</p>"},{"location":"api/models/#outlines.models.openai.build_optimistic_mask--parameters","title":"Parameters","text":"<p>transposed     A list of lists that contain the nth token of each choice.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def build_optimistic_mask(\n    transposed: List[Set[int]], max_mask_size: int = 300\n) -&gt; Dict[int, int]:\n    \"\"\"We build the largest mask possible.\n\n    Tokens are added from left to right, so if the encoded choices are e.g.\n    `[[1,2], [3,4]]`, `1` and `3` will be added before `2` and `4`.\n\n    Parameters\n    ----------\n    transposed\n        A list of lists that contain the nth token of each choice.\n\n    \"\"\"\n    mask: Dict[int, int] = {}\n    for tokens in transposed:\n        for token in tokens:\n            if len(mask) == max_mask_size:\n                return mask\n            mask[token] = 100\n\n    return mask\n</code></pre>"},{"location":"api/models/#outlines.models.openai.error_handler","title":"<code>error_handler(api_call_fn)</code>","text":"<p>Handle OpenAI API errors and missing API key.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def error_handler(api_call_fn: Callable) -&gt; Callable:\n    \"\"\"Handle OpenAI API errors and missing API key.\"\"\"\n\n    def call(*args, **kwargs):\n        import openai\n\n        try:\n            return api_call_fn(*args, **kwargs)\n        except (\n            openai.APITimeoutError,\n            openai.InternalServerError,\n            openai.RateLimitError,\n        ) as e:\n            raise OSError(f\"Could not connect to the OpenAI API: {e}\")\n        except (\n            openai.AuthenticationError,\n            openai.BadRequestError,\n            openai.ConflictError,\n            openai.PermissionDeniedError,\n            openai.NotFoundError,\n            openai.UnprocessableEntityError,\n        ) as e:\n            raise e\n\n    return call\n</code></pre>"},{"location":"api/models/#outlines.models.openai.find_longest_intersection","title":"<code>find_longest_intersection(response, choice)</code>","text":"<p>Find the longest intersection between the response and the choice.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def find_longest_intersection(response: List[int], choice: List[int]) -&gt; List[int]:\n    \"\"\"Find the longest intersection between the response and the choice.\"\"\"\n    for i, (token_r, token_c) in enumerate(zip_longest(response, choice)):\n        if token_r != token_c:\n            return response[:i]\n\n    return response\n</code></pre>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection","title":"<code>find_response_choices_intersection(response, choices)</code>","text":"<p>Find the longest intersection between the response and the different choices.</p> <p>Say the response is of the form <code>[1, 2, 3, 4, 5]</code> and we have the choices <code>[[1, 2], [1, 2, 3], [6, 7, 8]</code> then the function will return <code>[1, 2, 3]</code> as the intersection, and <code>[[]]</code> as the list of choices left.</p>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection--parameters","title":"Parameters","text":"<p>response     The model's response choices     The remaining possible choices</p>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection--returns","title":"Returns","text":"<p>A tuple that contains the longest intersection between the response and the different choices, and the choices which start with this intersection, with the intersection removed.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def find_response_choices_intersection(\n    response: List[int], choices: List[List[int]]\n) -&gt; Tuple[List[int], List[List[int]]]:\n    \"\"\"Find the longest intersection between the response and the different\n    choices.\n\n    Say the response is of the form `[1, 2, 3, 4, 5]` and we have the choices\n    `[[1, 2], [1, 2, 3], [6, 7, 8]` then the function will return `[1, 2, 3]` as the\n    intersection, and `[[]]` as the list of choices left.\n\n    Parameters\n    ----------\n    response\n        The model's response\n    choices\n        The remaining possible choices\n\n    Returns\n    -------\n    A tuple that contains the longest intersection between the response and the\n    different choices, and the choices which start with this intersection, with the\n    intersection removed.\n\n    \"\"\"\n    max_len_prefix = 0\n    choices_left = []\n    longest_prefix = []\n    for i, choice in enumerate(choices):\n        # Find the longest intersection between the response and the choice.\n        prefix = find_longest_intersection(response, choice)\n\n        if len(prefix) &gt; max_len_prefix:\n            max_len_prefix = len(prefix)\n            choices_left = [choice[len(prefix) :]]\n            longest_prefix = prefix\n\n        elif len(prefix) == max_len_prefix:\n            choices_left.append(choice[len(prefix) :])\n\n    return longest_prefix, choices_left\n</code></pre>"},{"location":"api/models/#outlines.models.openai.generate_chat","title":"<code>generate_chat(prompt, system_prompt, client, config)</code>  <code>async</code>","text":"<p>Call OpenAI's Chat Completion API.</p>"},{"location":"api/models/#outlines.models.openai.generate_chat--parameters","title":"Parameters","text":"<p>prompt     The prompt we use to start the generation. Passed to the model     with the \"user\" role. system_prompt     The system prompt, passed to the model with the \"system\" role     before the prompt. client     The API client config     An <code>OpenAIConfig</code> instance.</p>"},{"location":"api/models/#outlines.models.openai.generate_chat--returns","title":"Returns","text":"<p>A tuple that contains the model's response(s) and usage statistics.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@cache(ignore=\"client\")\n@functools.partial(vectorize, signature=\"(),(),()-&gt;(s)\")\nasync def generate_chat(\n    prompt: str,\n    system_prompt: Union[str, None],\n    client: \"AsyncOpenAI\",\n    config: OpenAIConfig,\n) -&gt; Tuple[np.ndarray, Dict]:\n    \"\"\"Call OpenAI's Chat Completion API.\n\n    Parameters\n    ----------\n    prompt\n        The prompt we use to start the generation. Passed to the model\n        with the \"user\" role.\n    system_prompt\n        The system prompt, passed to the model with the \"system\" role\n        before the prompt.\n    client\n        The API client\n    config\n        An `OpenAIConfig` instance.\n\n    Returns\n    -------\n    A tuple that contains the model's response(s) and usage statistics.\n\n    \"\"\"\n    system_message = (\n        [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n    )\n    user_message = [{\"role\": \"user\", \"content\": prompt}]\n\n    responses = await client.chat.completions.create(\n        messages=system_message + user_message,\n        **asdict(config),  # type: ignore\n    )\n\n    results = np.array([responses.choices[i].message.content for i in range(config.n)])\n\n    return results, responses.usage.model_dump()\n</code></pre>"},{"location":"api/parsing/","title":"Parsing","text":""},{"location":"api/parsing/#outlines.fsm.parsing.PartialIndenter","title":"<code>PartialIndenter</code>","text":"<p>             Bases: <code>Indenter</code></p> <p>An <code>Indenter</code> that doesn't reset its state every time <code>process</code> is called.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialIndenter(Indenter):\n    \"\"\"An `Indenter` that doesn't reset its state every time `process` is called.\"\"\"\n\n    def process(self, stream):\n        return self._process(stream)\n\n    def _process(self, stream):\n        for token in stream:\n            # These were previously *after* the `yield`, but that makes the\n            # state tracking unnecessarily convoluted.\n            if token.type in self.OPEN_PAREN_types:\n                self.paren_level += 1\n            elif token.type in self.CLOSE_PAREN_types:\n                self.paren_level -= 1\n                if self.paren_level &lt; 0:\n                    raise UnexpectedToken(token, [])\n\n            if token.type == self.NL_type:\n                yield from self.handle_NL(token)\n            else:\n                yield token\n\n        # TODO: What do we want to do here?\n        # while len(self.indent_level) &gt; 1:\n        #     self.indent_level.pop()\n        #     yield Token(self.DEDENT_type, \"\")\n\n    def accepts_token_type(self, token_type):\n        if token_type in self.CLOSE_PAREN_types and self.paren_level - 1 &lt; 0:\n            return False\n\n        # TODO:\n        # if token_type == self.NL_type and self.paren_level == 0:\n        #     ...\n        #     return False\n\n        return True\n\n    def __copy__(self):\n        res = type(self)()\n        res.paren_level = self.paren_level\n        res.indent_level = copy(self.indent_level)\n        return res\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(paren_level={self.paren_level!r}, indent_level={self.indent_level!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState","title":"<code>PartialParserState</code>","text":"<p>             Bases: <code>ParserState</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParserState(ParserState):\n    __slots__ = \"use_value_stack\"\n\n    def __init__(\n        self,\n        parse_conf,\n        lexer,\n        state_stack=None,\n        value_stack=None,\n        use_value_stack=False,\n    ):\n        super().__init__(\n            parse_conf, lexer, state_stack=state_stack, value_stack=value_stack\n        )\n        self.use_value_stack = use_value_stack\n\n    def feed_token(self, token, is_end=False):\n        if token.type == \"partial\":\n            # If none of the potential terminals can transition, we need to know now\n            current_state = self.state_stack[-1]\n            current_lexer = get_contextual_lexer(self.lexer).lexers[current_state]\n\n            # We have to feed the token and determine whether or not at least\n            # one terminal is consistent with the stack; otherwise, we'll miss\n            # invalid REDUCE cases.\n            # TODO: We should track separate parses conditional on possible\n            # token/symbol types, then we can coherently reuse the following\n            # results instead of recomputing it later.\n            can_transition = False\n            for terminal_info in token.value.terminals_and_info:\n                if terminal_info.terminal_name not in current_lexer.ignore_types:\n                    test_token = Token.new_borrow_pos(\n                        terminal_info.terminal_name, \"\", token\n                    )\n\n                    stack = copy(self.state_stack)\n                    try:\n                        self.feed_token_no_stack(test_token, is_end=is_end)\n                        can_transition = True\n                        break\n                    except UnexpectedToken:\n                        continue\n                    finally:\n                        self.state_stack = stack\n                else:\n                    can_transition = True\n\n            if not can_transition:\n                expected = {\n                    s\n                    for s in self.parse_conf.states[current_state].keys()\n                    if s.isupper()\n                }\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n        elif self.use_value_stack:\n            super().feed_token(token, is_end=is_end)\n        else:\n            self.feed_token_no_stack(token, is_end=is_end)\n\n    def feed_token_no_stack(self, token, is_end=False):\n        \"\"\"\n        This is a copy of `ParserState.feed_token` with all the value stack\n        steps removed.  Since we're not exactly parsing in order to obtain a\n        CST or anything similar, we can avoid the growing expense of tracking\n        the parse tree.\n        \"\"\"\n        state_stack = self.state_stack\n        states = self.parse_conf.states\n        end_state = self.parse_conf.end_state\n\n        while True:\n            state = state_stack[-1]\n            try:\n                action, arg = states[state][token.type]\n            except KeyError:\n                expected = {s for s in states[state].keys() if s.isupper()}\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n            assert arg != end_state\n\n            if action is Shift:\n                # shift once and return\n                assert not is_end\n                state_stack.append(arg)\n                return\n            else:\n                # reduce+shift as many times as necessary\n                rule = arg\n                size = len(rule.expansion)\n                if size:\n                    del state_stack[-size:]\n\n                _action, new_state = states[state_stack[-1]][rule.origin.name]\n                assert _action is Shift\n                state_stack.append(new_state)\n\n                if is_end and state_stack[-1] == end_state:\n                    return\n\n    def __copy__(self):\n        return type(self)(\n            self.parse_conf,\n            copy(self.lexer),\n            copy(self.state_stack),\n            deepcopy(self.value_stack),\n            use_value_stack=self.use_value_stack,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(lexer={self.lexer!r}, state_stack={self.state_stack!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState.feed_token_no_stack","title":"<code>feed_token_no_stack(token, is_end=False)</code>","text":"<p>This is a copy of <code>ParserState.feed_token</code> with all the value stack steps removed.  Since we're not exactly parsing in order to obtain a CST or anything similar, we can avoid the growing expense of tracking the parse tree.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def feed_token_no_stack(self, token, is_end=False):\n    \"\"\"\n    This is a copy of `ParserState.feed_token` with all the value stack\n    steps removed.  Since we're not exactly parsing in order to obtain a\n    CST or anything similar, we can avoid the growing expense of tracking\n    the parse tree.\n    \"\"\"\n    state_stack = self.state_stack\n    states = self.parse_conf.states\n    end_state = self.parse_conf.end_state\n\n    while True:\n        state = state_stack[-1]\n        try:\n            action, arg = states[state][token.type]\n        except KeyError:\n            expected = {s for s in states[state].keys() if s.isupper()}\n            raise UnexpectedToken(\n                token, expected, state=self, interactive_parser=None\n            )\n\n        assert arg != end_state\n\n        if action is Shift:\n            # shift once and return\n            assert not is_end\n            state_stack.append(arg)\n            return\n        else:\n            # reduce+shift as many times as necessary\n            rule = arg\n            size = len(rule.expansion)\n            if size:\n                del state_stack[-size:]\n\n            _action, new_state = states[state_stack[-1]][rule.origin.name]\n            assert _action is Shift\n            state_stack.append(new_state)\n\n            if is_end and state_stack[-1] == end_state:\n                return\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParsingFrontend","title":"<code>PartialParsingFrontend</code>","text":"<p>             Bases: <code>ParsingFrontend</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParsingFrontend(ParsingFrontend):\n    def __init__(self, lexer_conf, parser_conf, options, parser=None):\n        assert parser_conf.parser_type == \"lalr\"\n\n        options._plugins[\"LALR_Parser\"] = PartialLALRParser\n        options._plugins[\"BasicLexer\"] = PartialBasicLexer\n        options._plugins[\"ContextualLexer\"] = PartialContextualLexer\n        options._plugins[\"LexerThread\"] = PartialLexerThread\n\n        super().__init__(lexer_conf, parser_conf, options, parser=parser)\n\n        if lexer_conf.postlex:\n            self.lexer = PartialPostLexConnector(self.lexer.lexer, lexer_conf.postlex)\n\n        self._termset_fsm_info = None\n        self._symbols_to_states: Optional[\n            Dict[str, Set[Tuple[ParseStateType, Action]]]\n        ] = None\n        self._reverse_shifts: Optional[\n            Dict[ParseStateType, Dict[str, Set[ParseStateType]]]\n        ] = None\n        # self._state_transition_map: Optional[\n        #     Dict[Tuple[ParseStateType, str], Set[ParseStateType]]\n        # ] = None\n\n    def _compute_maps(\n        self,\n    ):\n        \"\"\"Compute state transition and symbols-to-states maps.\"\"\"\n        self._reverse_shifts = {}\n        self._symbols_to_states = {}\n\n        parse_table = self.parser.parser.parse_table\n\n        for from_state, symbols_to_ops in parse_table.states.items():\n            for symbol, op in symbols_to_ops.items():\n                if op[0] == Shift:\n                    symbols_to_from_states = self._reverse_shifts.setdefault(op[1], {})\n                    symbols_to_from_states.setdefault(symbol, set()).add(from_state)\n                self._symbols_to_states.setdefault(symbol, set()).add((from_state, op))\n\n        # # TODO: This approach is very wasteful.\n        # context_lexer = get_contextual_lexer(self)\n        # self._state_transition_map = {}\n        #\n        # for from_state, transitions in parse_table.states.items():\n        #     for symbol, action in transitions.items():\n        #         # TODO: Filter non-terminals\n        #         if symbol not in context_lexer.root_lexer.terminals_by_name:\n        #             continue\n        #\n        #         if action[0] is Shift:\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(action[1])\n        #             continue\n        #\n        #         antecedent_state_seqs = parse_to_terminal(self, [(from_state,)], symbol)\n        #\n        #         for antecedent_state_seq in antecedent_state_seqs:\n        #             antecedent_state = antecedent_state_seq[-1]\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(antecedent_state)\n\n    def _compute_termset_fsm_info(self):\n        \"\"\"Collect and return information about terminal symbol sets and their FSMs.\n\n        Terminal symbol sets (or \"termsets\") are ordered sequences of terminal\n        symbols that are used by each parser state.  Associated with each is a\n        collection of FSMs for each terminal and a single parse state FSM that is\n        the union of each terminal's FSM.\n\n        This constructs a list of tuples containing the termset, the set of\n        parse states that use the termsets, parse state FSMs, and information\n        mapping the components of the parse state FSMs to their terminal symbol\n        FSMs.\n\n        \"\"\"\n        context_lexer = get_contextual_lexer(self)\n        termsets_to_fsms = {}\n        termsets_to_parse_states: Dict[Tuple[str, ...], Set[ParseStateType]] = {}\n        for parse_state, lexer in context_lexer.lexers.items():\n            scanner = lexer.scanner\n            key = tuple(term.name for term in scanner.terminals)\n            termsets_to_fsms[key] = (scanner.fsm, scanner.fsms_to_trans_finals)\n            termsets_to_parse_states.setdefault(key, set()).add(parse_state)\n\n        self._termset_fsm_info = [\n            (\n                termset,\n                frozenset(termsets_to_parse_states[termset]),\n                fsm,\n                fsms_to_trans_finals,\n            )\n            for termset, (fsm, fsms_to_trans_finals) in termsets_to_fsms.items()\n        ]\n\n    @property\n    def termset_fsm_info(self):\n        if self._termset_fsm_info is None:\n            self._compute_termset_fsm_info()\n        return self._termset_fsm_info\n\n    @property\n    def symbols_to_states(self):\n        if self._symbols_to_states is None:\n            self._compute_maps()\n        return self._symbols_to_states\n\n    @property\n    def reverse_shifts(self):\n        if self._reverse_shifts is None:\n            self._compute_maps()\n        return self._reverse_shifts\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner","title":"<code>PartialScanner</code>","text":"<p>             Bases: <code>Scanner</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialScanner(Scanner):\n    @classmethod\n    @lru_cache\n    def construct_terminal_fsm(cls, terminal):\n        # TODO: This should really be done at the lexer/parser level so that\n        # the lifetime of these objects is tied to the parser itself.\n        regex_str = terminal.pattern.to_regexp()\n        pattern = interegular.parse_pattern(regex_str)\n        fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        return fsm, pattern.prefix_postfix\n\n    def __init__(self, terminals, g_regex_flags, re_, use_bytes, match_whole=False):\n        self.terminals = terminals\n        self.g_regex_flags = g_regex_flags\n        self.use_bytes = use_bytes\n        self.match_whole = match_whole\n        self.allowed_types = {t.name for t in self.terminals}\n        self._mres = None\n\n        fsms = []\n        for t in self.terminals:\n            fsm, prefix_postfix = self.construct_terminal_fsm(t)\n\n            # TODO FIXME: We don't support this right now.\n            assert prefix_postfix == (0, 0)\n\n            fsms.append(fsm)\n\n        self.fsm, self.fsms_to_trans_finals = fsm_union(fsms)\n\n    def get_terminals_info(\n        self, fsm_state_seq\n    ) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n        \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n        terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n            get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n        ):\n            terminal_name = self.terminals[fsm_id].name\n            info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n            terminals_and_info += (info,)\n            if in_final:\n                final_terminals_and_info += (info,)\n\n        return terminals_and_info, final_terminals_and_info\n\n    def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n        \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n        start_pos = pos\n\n        if last_fsm_state_seq:\n            assert len(last_fsm_state_seq) &gt; 1\n            start_pos += len(last_fsm_state_seq) - 1\n            start_state = last_fsm_state_seq[-1]\n        else:\n            start_state = self.fsm.initial\n\n        text_part = text[start_pos:]\n\n        state_seq = walk_fsm(\n            self.fsm,\n            text_part,\n            start_state,\n            full_match=self.match_whole,\n        )\n\n        if not state_seq:\n            return None\n\n        if last_fsm_state_seq:\n            res = last_fsm_state_seq + tuple(state_seq)\n        else:\n            res = (start_state,) + tuple(state_seq)\n\n        return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.get_terminals_info","title":"<code>get_terminals_info(fsm_state_seq)</code>","text":"<p>Get the possible terminal symbols for an FSM state sequence.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def get_terminals_info(\n    self, fsm_state_seq\n) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n    \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n    terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n        get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n    ):\n        terminal_name = self.terminals[fsm_id].name\n        info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n        terminals_and_info += (info,)\n        if in_final:\n            final_terminals_and_info += (info,)\n\n    return terminals_and_info, final_terminals_and_info\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.match","title":"<code>match(text, pos, last_fsm_state_seq=None)</code>","text":"<p>Determine an FSM match over <code>text</code> starting at <code>pos</code> and continuing <code>last_fsm_state_seq</code>.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n    \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n    start_pos = pos\n\n    if last_fsm_state_seq:\n        assert len(last_fsm_state_seq) &gt; 1\n        start_pos += len(last_fsm_state_seq) - 1\n        start_state = last_fsm_state_seq[-1]\n    else:\n        start_state = self.fsm.initial\n\n    text_part = text[start_pos:]\n\n    state_seq = walk_fsm(\n        self.fsm,\n        text_part,\n        start_state,\n        full_match=self.match_whole,\n    )\n\n    if not state_seq:\n        return None\n\n    if last_fsm_state_seq:\n        res = last_fsm_state_seq + tuple(state_seq)\n    else:\n        res = (start_state,) + tuple(state_seq)\n\n    return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.terminals_to_fsms","title":"<code>terminals_to_fsms(lp)</code>","text":"<p>Construct a <code>dict</code> mapping terminal symbol names to their finite state machines.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def terminals_to_fsms(lp: PartialLark) -&gt; Dict[str, FSM]:\n    \"\"\"Construct a ``dict`` mapping terminal symbol names to their finite state machines.\"\"\"\n\n    symbol_names_and_fsms = {}\n    for terminal in lp.terminals:\n        pattern = interegular.parse_pattern(terminal.pattern.to_regexp())\n        # TODO: Use `pyparser.terminals[0].pattern.flags`?\n        try:\n            fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        except Unsupported:\n            fsm = None\n\n        symbol_names_and_fsms[terminal.name] = fsm\n\n    return symbol_names_and_fsms\n</code></pre>"},{"location":"api/prompts/","title":"Prompts","text":""},{"location":"api/prompts/#outlines.prompts.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>Represents a prompt function.</p> <p>We return a <code>Prompt</code> class instead of a simple function so the template defined in prompt functions can be accessed.</p> Source code in <code>outlines/prompts.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"Represents a prompt function.\n\n    We return a `Prompt` class instead of a simple function so the\n    template defined in prompt functions can be accessed.\n\n    \"\"\"\n\n    template: str\n    signature: inspect.Signature\n\n    def __post_init__(self):\n        self.parameters: List[str] = list(self.signature.parameters.keys())\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        The rendered template as a Python ``str``.\n\n        \"\"\"\n        bound_arguments = self.signature.bind(*args, **kwargs)\n        bound_arguments.apply_defaults()\n        return render(self.template, **bound_arguments.arguments)\n\n    def __str__(self):\n        return self.template\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.Prompt.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p>"},{"location":"api/prompts/#outlines.prompts.Prompt.__call__--returns","title":"Returns","text":"<p>The rendered template as a Python <code>str</code>.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    The rendered template as a Python ``str``.\n\n    \"\"\"\n    bound_arguments = self.signature.bind(*args, **kwargs)\n    bound_arguments.apply_defaults()\n    return render(self.template, **bound_arguments.arguments)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/prompts.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/prompts.py</code> <pre><code>@get_schema.register(type(BaseModel))\ndef get_schema_pydantic(model: Type[BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if not type(model) == type(BaseModel):\n        raise TypeError(\"The `schema` filter only applies to Pydantic models.\")\n\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value:\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.prompt","title":"<code>prompt(fn)</code>","text":"<p>Decorate a function that contains a prompt template.</p> <p>This allows to define prompts in the docstring of a function and simplify their manipulation by providing some degree of encapsulation. It uses the <code>render</code> function internally to render templates.</p> <p>import outlines</p> <p>@outlines.prompt def build_prompt(question): ...    \"I have a ${question}\" ... prompt = build_prompt(\"How are you?\")</p> <p>This API can also be helpful in an \"agent\" context where parts of the prompt are set when the agent is initialized and never modified later. In this situation we can partially apply the prompt function at initialization.</p> <p>import outlines import functools as ft ... @outlines.prompt ... def solve_task(name: str, objective: str, task: str): ...     '''Your name is {{name}}. ..      Your overall objective is to {{objective}}. ...     Please solve the following task: {{task}} ...     ''' ... hal = ft.partial(solve_taks, \"HAL\", \"Travel to Jupiter\")</p>"},{"location":"api/prompts/#outlines.prompts.prompt--returns","title":"Returns","text":"<p>A <code>Prompt</code> callable class which will render the template when called.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def prompt(fn: Callable) -&gt; Prompt:\n    \"\"\"Decorate a function that contains a prompt template.\n\n    This allows to define prompts in the docstring of a function and simplify their\n    manipulation by providing some degree of encapsulation. It uses the `render`\n    function internally to render templates.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @outlines.prompt\n    &gt;&gt;&gt; def build_prompt(question):\n    ...    \"I have a ${question}\"\n    ...\n    &gt;&gt;&gt; prompt = build_prompt(\"How are you?\")\n\n    This API can also be helpful in an \"agent\" context where parts of the prompt\n    are set when the agent is initialized and never modified later. In this situation\n    we can partially apply the prompt function at initialization.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; import functools as ft\n    ...\n    &gt;&gt;&gt; @outlines.prompt\n    ... def solve_task(name: str, objective: str, task: str):\n    ...     '''Your name is {{name}}.\n    ..      Your overall objective is to {{objective}}.\n    ...     Please solve the following task: {{task}}\n    ...     '''\n    ...\n    &gt;&gt;&gt; hal = ft.partial(solve_taks, \"HAL\", \"Travel to Jupiter\")\n\n    Returns\n    -------\n    A `Prompt` callable class which will render the template when called.\n\n    \"\"\"\n\n    signature = inspect.signature(fn)\n\n    # The docstring contains the template that will be rendered to be used\n    # as a prompt to the language model.\n    docstring = fn.__doc__\n    if docstring is None:\n        raise TypeError(\"Could not find a template in the function's docstring.\")\n\n    template = cast(str, docstring)\n\n    return Prompt(template, signature)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.render","title":"<code>render(template, **values)</code>","text":"<p>Parse a Jinaj2 template and translate it into an Outlines graph.</p> <p>This function removes extra whitespaces and linebreaks from templates to allow users to enter prompts more naturally than if they used Python's constructs directly. See the examples for a detailed explanation.</p>"},{"location":"api/prompts/#outlines.prompts.render--examples","title":"Examples","text":"<p>Outlines follow Jinja2's syntax</p> <p>import outlines outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\") I like tomatoes and tennis</p> <p>If the first line of the template is empty, <code>render</code> removes it</p> <p>from outlines import render</p> <p>tpl = ''' ... A new string''' tpl ... '\\nA new string' render(tpl) ... 'a new string'</p> <p>Similarly, <code>render</code> ignores linebreaks introduced by placing the closing quotes underneath the text:</p> <p>tpl = ''' ... A new string ... ''' tpl ... '\\nA new string\\n' render(tpl) ... 'A new string'</p> <p>If you want to insert a linebreak at the end of the rendered template, you will need to leave an empty line at the end of the template:</p> <p>tpl = ''' ... A new string ... ... ''' tpl ... '\\nA new string\\n\\n' render(tpl) ... 'A new string\\n'</p> <p><code>render</code> removes the identation in docstrings. This is particularly important when using prompt functions</p> <p>tpl = ''' ...    a string ...    and another string''' tpl ... '\\n   a string\\n   and another string' render(tpl) ... 'a string\\nand another string'</p> <p>The indentation of the first line is assumed to be the same as the second line's</p> <p>tpl = '''a string ...     and another''' tpl ... 'a string\\n    and another' render(tpl) ... 'a string\\nand another'</p> <p>To get a different indentation for the first and the second line, we can start the prompt on the string's second line:</p> <p>tpl = ''' ... First line ...   Second line''' render(tpl) ... 'First Line\\n  Second Line'</p>"},{"location":"api/prompts/#outlines.prompts.render--parameters","title":"Parameters","text":"<p>template     A string that contains a template written with the Jinja2 syntax. **values     Map from the variables in the template to their value.</p>"},{"location":"api/prompts/#outlines.prompts.render--returns","title":"Returns","text":"<p>A string that contains the rendered template.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def render(template: str, **values: Optional[Dict[str, Any]]) -&gt; str:\n    r\"\"\"Parse a Jinaj2 template and translate it into an Outlines graph.\n\n    This function removes extra whitespaces and linebreaks from templates to\n    allow users to enter prompts more naturally than if they used Python's\n    constructs directly. See the examples for a detailed explanation.\n\n    Examples\n    --------\n\n    Outlines follow Jinja2's syntax\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\")\n    I like tomatoes and tennis\n\n    If the first line of the template is empty, `render` removes it\n\n    &gt;&gt;&gt; from outlines import render\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tpl = '''\n    ... A new string'''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a new string'\n\n    Similarly, `render` ignores linebreaks introduced by placing the closing quotes\n    underneath the text:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string'\n\n    If you want to insert a linebreak at the end of the rendered template, you will\n    need to leave an empty line at the end of the template:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ...\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string\\n'\n\n    `render` removes the identation in docstrings. This is particularly important\n    when using prompt functions\n\n    &gt;&gt;&gt; tpl = '''\n    ...    a string\n    ...    and another string'''\n    &gt;&gt;&gt; tpl\n    ... '\\n   a string\\n   and another string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another string'\n\n    The indentation of the first line is assumed to be the same as the second line's\n\n    &gt;&gt;&gt; tpl = '''a string\n    ...     and another'''\n    &gt;&gt;&gt; tpl\n    ... 'a string\\n    and another'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another'\n\n    To get a different indentation for the first and the second line, we can start the\n    prompt on the string's second line:\n\n    &gt;&gt;&gt; tpl = '''\n    ... First line\n    ...   Second line'''\n    &gt;&gt;&gt; render(tpl)\n    ... 'First Line\\n  Second Line'\n\n    Parameters\n    ----------\n    template\n        A string that contains a template written with the Jinja2 syntax.\n    **values\n        Map from the variables in the template to their value.\n\n    Returns\n    -------\n    A string that contains the rendered template.\n\n    \"\"\"\n    # Dedent, and remove extra linebreak\n    cleaned_template = inspect.cleandoc(template)\n\n    # Add linebreak if there were any extra linebreaks that\n    # `cleandoc` would have removed\n    ends_with_linebreak = template.replace(\" \", \"\").endswith(\"\\n\\n\")\n    if ends_with_linebreak:\n        cleaned_template += \"\\n\"\n\n    # Remove extra whitespaces, except those that immediately follow a newline symbol.\n    # This is necessary to avoid introducing whitespaces after backslash `\\` characters\n    # used to continue to the next line without linebreak.\n    cleaned_template = re.sub(r\"(?![\\r\\n])(\\b\\s+)\", \" \", cleaned_template)\n\n    env = Environment(\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True,\n        undefined=StrictUndefined,\n    )\n    env.filters[\"name\"] = get_fn_name\n    env.filters[\"description\"] = get_fn_description\n    env.filters[\"source\"] = get_fn_source\n    env.filters[\"signature\"] = get_fn_signature\n    env.filters[\"schema\"] = get_schema\n\n    jinja_template = env.from_string(cleaned_template)\n\n    return jinja_template.render(**values)\n</code></pre>"},{"location":"api/regex/","title":"Regex","text":"Source code in <code>outlines/generate/api.py</code> <pre><code>def regex(\n    model,\n    regex_str: str,\n    max_tokens: Optional[int] = None,\n    sampler: Sampler = multinomial,\n):\n    fsm = RegexFSM(regex_str, model.tokenizer, max_tokens)\n\n    device = model.device\n    generator = SequenceGenerator(fsm, model, sampler, device)\n\n    return generator\n</code></pre>"},{"location":"api/samplers/","title":"Samplers","text":""},{"location":"api/samplers/#outlines.generate.samplers.greedy","title":"<code>greedy(logits, samples, *_)</code>","text":"<p>Greedy Sampling algorithm.</p> <p>Greedy sampling consists in choosing the token with the largest likelihood at every step.</p>"},{"location":"api/samplers/#outlines.generate.samplers.greedy--parameters","title":"Parameters","text":"<p>logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. samples     The number of sequences to produce.  In this case, the top-<code>samples</code>     logit values are returned. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.generate.samplers.greedy--returns","title":"Returns","text":"<p>The ids of the sampled tokens having shape <code>(samples, n_seqs)</code>.</p> Source code in <code>outlines/generate/samplers.py</code> <pre><code>def greedy(logits: torch.DoubleTensor, samples: int, *_) -&gt; torch.DoubleTensor:\n    \"\"\"Greedy Sampling algorithm.\n\n    Greedy sampling consists in choosing the token with the largest\n    likelihood at every step.\n\n    Parameters\n    ----------\n    logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    samples\n        The number of sequences to produce.  In this case, the top-`samples`\n        logit values are returned.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    The ids of the sampled tokens having shape ``(samples, n_seqs)``.\n\n    \"\"\"\n    if samples == 1:\n        next_token_ids = torch.argmax(logits, dim=-1, keepdim=True).T\n    else:\n        next_token_ids = torch.topk(\n            logits, samples, dim=-1, largest=True, sorted=True\n        ).indices.T\n\n    return next_token_ids\n</code></pre>"},{"location":"api/samplers/#outlines.generate.samplers.multinomial","title":"<code>multinomial(logits, samples, rng)</code>","text":"<p>Multinomial sampling algorithm.</p> <p>Multinomial sampling consists in randomly sampling the next token assuming its distribution is a Categorical distribution parametrized by the next-token logits.</p>"},{"location":"api/samplers/#outlines.generate.samplers.multinomial--parameters","title":"Parameters","text":"<p>logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. samples     The number of sequences to sample. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.generate.samplers.multinomial--returns","title":"Returns","text":"<p>The ids of the sampled tokens having shape <code>(samples, n_seqs)</code>.</p> Source code in <code>outlines/generate/samplers.py</code> <pre><code>def multinomial(\n    logits: torch.DoubleTensor, samples: int, rng: torch.Generator\n) -&gt; torch.DoubleTensor:\n    \"\"\"Multinomial sampling algorithm.\n\n    Multinomial sampling consists in randomly sampling the next token assuming\n    its distribution is a Categorical distribution parametrized by the\n    next-token logits.\n\n    Parameters\n    ----------\n    logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    samples\n        The number of sequences to sample.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    The ids of the sampled tokens having shape ``(samples, n_seqs)``.\n\n    \"\"\"\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    next_token_ids = torch.multinomial(probs, num_samples=samples, generator=rng)\n    return next_token_ids\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<ul> <li>Dating Profile: Build dating profiles from descriptions using prompt templating and JSON-guided generation.</li> <li>Chain Of Density: Summarize documents using chain of density prompting and JSON-guided generation.</li> <li>Playing Chess: Make Mistral-7B play chess against itself using regex-guided generation.</li> </ul>"},{"location":"examples/chain_of_density/","title":"Summarize documents using Chain of Density prompting","text":"<p>A good summary should be informative, concise and clear. While large language models are generally good at summarizing documents, their summaries tend to be long and contain redundant information; their information density tends to be on the lower end. This is where chain of Density, a new prompting technique, comes in. In this example we will show how one can implement chain of density with a few lines of code using Outlines, leveraging both Outline's prompt templating and its guided generation capabilities.</p> <p>The article we will try to summarize is the first three paragraphs of the Alan Turing page on Wikipedia:</p> <pre><code>article = \"\"\"\nAlan Mathison Turing OBE FRS (/\u02c8tj\u028a\u0259r\u026a\u014b/; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[5] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[6][7][8] He is widely considered to be the father of theoretical computer science and artificial intelligence.[9]\n\nBorn in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes\u2013no questions can never be answered by computation. He defined a Turing machine and proved that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.[10][11]\n\nAfter the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[12] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.[13]\n\"\"\"\n</code></pre>"},{"location":"examples/chain_of_density/#how-chain-of-density-works","title":"How Chain Of Density works","text":"<p>Chain Of Density starts with asking the model to generate a first long and non-specific summary. Then it asks the model to generate 4 extra summaries by proceeding in the following way:</p> <ol> <li>Identify 1-3 entities missing in the previous summary;</li> <li>Add all entities marked as missing in the previous step, while not dropping entities;</li> <li>Make the summary more concise;</li> </ol> <p>The prompt also asks the model to return a list of JSON objects that contain the missing entities and the new summary. This is where guided generation will come in handy :) The paper provides the prompt and an example:</p> <p></p> <p>We can now implement the prompt provided in the paper:</p> <pre><code>import outlines\n\n@outlines.prompt\ndef chain_of_density(article):\n    \"\"\"Article: {{ article }}\n\n    You will generate increasingly concise, entity-dense summaries of the above Article.\n\n    Repeat the following 2 steps 5 times.\n\n    Step 1. Identify 1-3 informative Entities (\"; \" delimited) from the Article which are missing from the previously generated summary.\n    Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n\n    A Missing Entity is:\n    - Relevant: to the main story.\n    - Specific: descriptive yet concise (5 words or fewer).\n    - Novel: not in the previous summary.\n    - Faithful: present in the Article.\n    - Anywhere: located anywhere in the Article.\n\n    Guidelines:\n    - The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n    - Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n    - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n    - Missing entities can appear anywhere in the new summary.\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\n    Remember, use the exact same number of words for each summary.\n\n    Answer in JSON. The JSON should be a a dictionary with key \"summaries\" that contains a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n    \"\"\"\n</code></pre> Note <p>Note that we modified the prompt slightly so it returns a JSON object that contains the summaries, instead of a list of summaries.</p>"},{"location":"examples/chain_of_density/#outlines-implementation","title":"Outlines implementation","text":"<p>We will use Outline's JSON-guided generation to ensure that the model's output is consistent with the format specified in the prompt. We start with defining the JSON objects that the model is asked to return using Pydantic. One JSON object that contains a list of <code>Summary</code> objects that contain the missing entities and new summary:</p> <pre><code>from pydantic import BaseModel, conlist\n\nclass Summary(BaseModel):\n    missing_entities: str\n    denser_summary: str\n\nclass Summaries(BaseModel):\n    summaries: conlist(Summary, max_length=5, min_length=5)\n</code></pre> <p>We now generate the prompt by passing the article we want to summarize to the template. We load a quantized version of Mistral-7B using the AutoAWQ library, and then use JSON-guided generation to generate the summaries:</p> <pre><code>model = outlines.models.awq(\"TheBloke/Mistral-7B-OpenOrca-AWQ\")\n\nprompt = chain_of_density(article)\nresult = outlines.generate.json(model, Summaries)(prompt)\n</code></pre> <p>We can now check the results:</p> <pre><code>print(result.model_dump())\n# {'summaries': [\n#     {\n#       'missing_entities': 'English mathematician, cryptanalyst, philosopher',\n#       'denser_summary': 'Alan Mathison Turing was an English mathematician, cryptanalyst, philosopher.'\n#     },\n#     {\n#       'missing_entities': '',\n#       'denser_summary': \"Alan Mathison Turing was an English mathematician who was a crucial figure in WW2's Bletchley Park codebreaking centre and designed one of the first computers.\"\n#     },\n#     {\n#       'missing_entities': 'cryptanalyst, studied, biology, father',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and contributed to mathematical biology.'\n#     },\n#     {\n#       'missing_entities': 'biology, morphogenesis, chemical',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and predicted chemical reactions in morphogenesis.\n#     '},\n#     {\n#       'missing_entities': '',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, developed computer science, and made strides in mathematical biology research.'\n#       }\n# ]}\n</code></pre> <p>Not bad, considering we used a smallish model to generate the summary! Chain of Density seems to be a very effective prompting technique to generate dense summaries, even with small quantized models. Its implementation in Outlines is also very short.</p> <p>Note that this is the first article I tried and it worked out of the box. Try it out on other articles, and please share the results on Twitter, or by opening a new discussion on the Outlines repository!</p>"},{"location":"examples/dating_profiles/","title":"Generate a synthetic dating profile from a description","text":"<p>In this example we will see how we can use Outlines to generate synthetic data for a dating application. This example was originally contributed by Vibhor Kumar.</p> <pre><code>from dataclasses import dataclass\nfrom enum import Enum\n\nimport torch\nimport transformers\nfrom pydantic import BaseModel, conlist, constr\n\nimport outlines\n</code></pre>"},{"location":"examples/dating_profiles/#defining-the-profile-with-pydantic","title":"Defining the profile with Pydantic","text":"<p>Here a dating profile will consist in a biography, a job, a list of interests and two question-answer pairs. The questions are written in advance by the team, and the users are asked to provide an answer:</p> <pre><code>class QuestionChoice(str, Enum):\n    A = \"The key to my heart is\"\n    B = \"The first item on my bucket list is\"\n    C = \"Perks of dating me\"\n    D = \"Message me if you also love\"\n    E = \"People would describe me as\"\n    F = \"I can beat you in a game of\"\n\n@dataclass\nclass QuestionAnswer:\n    question: QuestionChoice\n    answer: str\n</code></pre> <p>Users need to provide a short biography, with a minimum of 10 and a maximum of 300 characters. The application also limits job descriptions to 50 characters. In addition to the question-answer pairs, the user is required to provide a list of between 1 and 5 interests:</p> <pre><code>class DatingProfile(BaseModel):\n    bio: constr(str, min_length=10, max_length=300)\n    job: constr(str, max_lengt=50)\n    interests: conlist(str, min_length=1, max_length=5)  # type: ignore\n    qna1: QuestionAnswer\n    qna2: QuestionAnswer\n</code></pre>"},{"location":"examples/dating_profiles/#prompt-template-and-examples","title":"Prompt template and examples","text":"<p>We will ask the model to generate profiles from a high-level description:</p> <pre><code>@dataclass\nclass Example:\n    description: str\n    profile: DatingProfile\n</code></pre> <p>We will use Outlines' prompt templating abilities to generate the prompt for us. This help clearly separate the general prompting logic from what is specific to an example.</p> <pre><code>@outlines.prompt\ndef dating_profile_prompt(description: str, examples: list[Example]):\n    \"\"\"\n    You are a world-renowned matchmaker who understands the modern dating\n    market. Your job is to generate dating app profiles for male clients\n    interested in women based on a provided description. The profiles should be\n    authentic, show off their strengths, and maximize their likelihood of\n    getting matches on dating apps.  Here are some examples of past clients that\n    you have successfully created profiles for:\n\n    {% for example in examples %}\n    Description:\n    {{ example.description }}\n    Profile:\n    {{ example.profile }}\n    {% endfor %}\n\n    Here is the new client who you need to create a profile for:\n    Description: {{ description }}\n    Profile:\n    \"\"\"\n</code></pre> <p>We will provide the model with several few-shot examples:</p> <pre><code>samples: list[Example] = [\n    Example(\n        description=\"I'm an author and former professional soccer player living in Seattle who publishes popular fiction books. A typical day for me starts by hanging out with my cat, drinking a coffee, and reading as much as I can in a few hours. Then, I'll prepare a quick smoothie before starting to write for a few hours, take a break with soccer or running a few miles, and finally meet friends for dinner at a new, hip restaurant in the evening. Sometimes we go axe-throwing afterwards, or play poker, or watch a comedy show, or visit a dive bar. On my vacations, I travel extensively to countries South America, Europe, and Asia, with the goal of visiting them all!\",\n        profile=DatingProfile(\n            bio=\"Adventurer, dreamer, author, and soccer enthusiast. Life\u2019s too short to waste time so I make the most of each day by exploring new places and playing with my friends on the pitch. What\u2019s your favorite way to get out and have fun?\",\n            job=\"Famous Soccer Player -&gt; Famous Author\",\n            interests=[\"Soccer\", \"Travel\", \"Friends\", \"Books\", \"Fluffy Animals\"],\n            qna1=QuestionAnswer(\n                question=QuestionChoice.B, answer=\"swim in all seven oceans!\"\n            ),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.E,\n                answer=\"fun-loving, adventurous, and a little bit crazy\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my company and build houses for a living. I'm a big fan of the outdoors and love to go hiking, camping, and fishing. I don't like video games, but do like to watch movies. My love language is home-cooked food, and I'm looking for someone who isn't afraid to get their hands dirty.\",\n        profile=DatingProfile(\n            bio=\"If you're looking for a Montana man who loves to get outdoors and hunt, and who's in-tune with his masculinity then I'm your guy!\",\n            job=\"House Construction Manager / Entrepreneur\",\n            interests=[\"Hunting\", \"Hiking\", \"The outdoors\", \"Home-cooked food\"],\n            qna1=QuestionAnswer(question=QuestionChoice.A, answer=\"food made at home\"),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.C,\n                answer=\"having a man in your life who can fix anything\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my own Youtube channel with 10M subscribers. I love working with kids, and my audience skews pretty young too. In my free time, I play Fortnite and Roblox. I'm looking for someone who is also a gamer and likes to have fun. I'm learning Japanese in my free time as well as how to cook.\",\n        profile=DatingProfile(\n            bio=\"Easy on the eyes (find me on Youtube!) and great with kids. What more do you need?\",\n            job=\"Youtuber 10M+ subscribers\",\n            interests=[\"Kids\", \"Gaming\", \"Japanese\"],\n            qna1=QuestionAnswer(question=QuestionChoice.D, answer=\"anime and gaming!\"),\n            qna2=QuestionAnswer(question=QuestionChoice.F, answer=\"Fortnite, gg ez\"),\n        ),\n    ),\n]\n</code></pre>"},{"location":"examples/dating_profiles/#load-the-model","title":"Load the model","text":"<p>We will use Mosaic's MPT-7B model (requires 13GB of GPU memory) which can fit on a single GPU with a reasonable context window. We initialize it with Outlines:</p> <pre><code>config = transformers.AutoConfig.from_pretrained(\n    \"mosaicml/mpt-7b-8k-instruct\", trust_remote_code=True\n)\nconfig.init_device = \"meta\"\nmodel = outlines.models.transformers(\n    model_name=\"mosaicml/mpt-7b-8k-instruct\",\n    device=\"cuda\",\n    model_kwargs={\n        \"config\": config,\n        \"trust_remote_code\": True,\n        \"torch_dtype\": torch.bfloat16,\n        \"device_map\": {\"\": 0},\n    },\n)\n</code></pre>"},{"location":"examples/dating_profiles/#json-guided-generation-of-profiles","title":"JSON-guided generation of profiles","text":"<p>We will now generate a dating profile from a textual description of oneself:</p> <pre><code>new_description = \"\"\"I'm a laid-back lawyer who spends a lot of his free-time\ngaming. I work in a corporate office, but ended up here after the start-up  I\ncofounded got acquired, so still play ping pong with my cool coworkers every\nday.  I have a bar at home where I make cocktails, which is great for\nentertaining  friends. I secretly like to wear suits and get a new one tailored\nevery few  months. I also like weddings because I get to wear those suits, and\nit's  a good excuse for a date. I watch the latest series because I'm paying,\nwith my hard-earned money, for every streaming service.\"\"\"\n\nprompt = dating_profile_prompt(new_description, samples)\nprofile = outlines.generate.json(model, DatingProfile)(prompt)\nparsed_profile = DatingProfile.model_validate_json(profile)\n</code></pre>"},{"location":"examples/dating_profiles/#results","title":"Results","text":"<p>Here are a couple of results:</p> <pre><code>{\n    \"bio\": \"\"\"I'm an ambitious lawyer with a casual and fashionable style. I love\n    games and sports, but my true passion is preparing refreshing cocktails at\n    home and dressing to the nines at weddings. I'm currently looking for a woman\n    to show a good time to and get a kiss on the opulent suit I just had made.\n    Send resume to this inbox.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Stylish guys\",\n        \"Gaming\",\n        \"Ping pong\",\n        \"Cocktails\",\n        \"Weddings\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"be married and have a family.\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"charming, stylish, and funny.\"\n    }\n}\n</code></pre> <pre><code>{\n    \"bio\": \"\"\"I\u2019m a sexy lawyer with time on my hands. I love to game and\n    play ping pong, but the real reason you should swipe to the right\n    is because I look great in a suit. Who doesn\u2019t love a man in a\n    suit? Just saying. Send me a message if you think it\u2019s time to take\n    your dating life to the next level.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Gaming\",\n        \"Ping Pong\",\n        \"Tailored Suits\",\n        \"Weddings\",\n        \"Streaming Services\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"simulate space but stay alive for as long as possible\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"easy-going, a little nerdy but with a mature essence\"\n    }\n}\n</code></pre>"},{"location":"examples/models_playing_chess/","title":"Large language models playing chess","text":"<p>In this example we will make a quantized version of Mistral-7B play chess against itself. On its own the model easily generates invalid move, so we will give it a little help. At each step we will generate a regex that only matches valid move, and use it to help the model only generating valid moves.</p>"},{"location":"examples/models_playing_chess/#the-chessboard","title":"The chessboard","text":"<p>The game will be played on a standard checkboard. We will use the <code>chess</code> library to track the opponents' moves, and check that the moves are valid.</p> <pre><code>import chess\n\nboard = chess.Board(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\")\n</code></pre>"},{"location":"examples/models_playing_chess/#the-opponents","title":"The opponents","text":"<p>Mistral-7B quantized will be playing against itself:</p> <pre><code>from outlines import models\n\nboard_state = models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre>"},{"location":"examples/models_playing_chess/#a-little-help-for-the-language-model","title":"A little help for the language model","text":"<p>To make sure Mistral-7B generates valid chess moves we will use Outline's regex-guided generation. We define a function that takes the current state of the board and returns a regex that matches all possible legal moves:</p> <pre><code>import re\n\ndef legal_moves_regex(board):\n    \"\"\"Build a regex that only matches valid moves.\"\"\"\n    legal_moves = list(board.legal_moves)\n    legal_modes_str = [board.san(move) for move in legal_moves]\n    legal_modes_str = [re.sub(r\"[+#]\", \"\", move) for move in legal_modes_str]\n    regex_pattern = \"|\".join(re.escape(move) for move in legal_modes_str)\n    regex_pattern = f\"{regex_pattern}\"\n    return regex_pattern\n</code></pre>"},{"location":"examples/models_playing_chess/#prompting-the-language-model","title":"Prompting the language model","text":"<p>The prompt corresponds to the current state of the board, so we start with:</p> <pre><code>prompt = \"Score: 1-0 WhiteElo: 1600 BlackElo: 1600 Timecontrol: 1800+0 Moves: 1.\"\n</code></pre> <p>We update the prompt at each step so it reflects the state of the board after the previous move.</p>"},{"location":"examples/models_playing_chess/#lets-play","title":"Let's play!","text":"<pre><code>from outlines import generate\n\n\nturn_number = 0\nwhile not board.is_game_over():\n    regex_pattern = legal_moves_regex(board)\n    guided = generate.regex(model, regex_pattern)(board_state)\n    move = board.parse_san(guided)\n\n    if turn_number % 2 == 0 :  # It's White's turn\n        board_state += board.san(move) + \" \"\n    else:\n        board_state += board.san(move) + \" \" + str(turn_number) + \".\"\n\n    turn_number += 1\n\n    board.push(move)\n\n    print(board_state)\n</code></pre> <p>It turns out Mistal-7B (quantized) is not very good at playing chess: the game systematically ends because of the threefold repetition rule.</p> <p>This example was originally authored by @903124S in this gist.</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#constrained-generation","title":"Constrained generation","text":"<p>While LLM capabilities are increasingly impressive, we can make their output more reliable by steering the generation. Outlines thus offers mechanisms to specify high level constraints on text completions by generative language models.</p> <p>Stopping sequence By default, language models stop generating tokens after and  token was generated, or after a set maximum number of tokens. Their output can be verbose, and for practical purposes it is often necessary to stop the generation after a given sequence has been found instead. You can use the stop_at keyword argument when calling the model with a prompt: <pre><code>import outlines.models as models\n\ncomplete = models.text_completion.openai(\"text-davinci-002\")\nexpert = complete(\"Name an expert in quantum gravity.\", stop_at=[\"\\n\", \".\"])\n</code></pre>"},{"location":"reference/choices/","title":"Multiple choices","text":"<p>Choice between different options In some cases we know the output is to be chosen between different options. We can restrict the completion\u2019s output to these choices using the is_in keyword argument:</p> <pre><code>import outlines.models as models\n\ncomplete = models.text_completion.openai(\"text-davinci-002\")\nanswer = complete(\n    \"Pick the odd word out: skirt, dress, pen, jacket\",\n    is_in=[\"skirt\", \"dress\", \"pen\", \"jacket\"]\n)\n</code></pre>"},{"location":"reference/json/","title":"Make the LLM follow a JSON Schema","text":"<p>Outlines can make any open source model return a JSON object that follows a structure that is specified by the user. This is useful whenever we want the output of the model to be processed by code downstream: code does not understand natural language but rather the structured language it has been programmed to understand.</p> <p>There are mostly two reasons why someone would want to get an output formatted as JSON from a LLM:</p> <ol> <li>Parse the answer (e.g. with Pydantic), store it somewhere, return it to a user, etc.</li> <li>Call a function with the result</li> </ol> <p>Outlines has you covered in both cases! Indeed, to define the structure of the JSON you want the model to follow you can either provide a Pydantic model, or a function. No need to duplicate code!</p>"},{"location":"reference/json/#using-pydantic","title":"Using Pydantic","text":"<p>Outlines can infer the structure of the output from a Pydantic model. The result is an instance of the model that contains the values returned by the LLM:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models\nfrom outlines import text\n\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B\")\ngenerator = text.generate.json(model, User)\nresult = generator(\"Create a user profile with the fields name, last_name and id\")\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre>"},{"location":"reference/json/#from-a-functions-signature","title":"From a function's signature","text":"<p>Outlines can infer the structure of the output from the signature of a function. The result is a dictionary, and can be passed directly to the function using the usual dictionary expansion syntax <code>**</code>:</p> <pre><code>from outlines import models\nfrom outlines import text\n\ndef add(a: int, b: int):\n    return a + b\n\nmodel = models.transformers(\"mistralai/Mistral-7B\")\ngenerator = text.generate.json(model, add)\nresult = generator(\"Return two integers named a and b respectively. a is odd and b even.\")\n\nprint(add(**result))\n# 3\n</code></pre> <p>A great advantage of passing functions directly to specify the structure is that the structure of the LLM will change with the function's definition. No need to change the code at several places!</p>"},{"location":"reference/openai_text_generation/","title":"Generate text with the OpenAI API","text":"<p>Outlines supports models available via the OpenAI Chat API, e.g. ChatGPT and GPT-4. The following models can be used with Outlines:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-3.5-turbo\")\nmodel = models.openai(\"gpt-4\")\n\nprint(type(model))\n# OpenAI\n</code></pre> <p>It is possible to pass a system message to the model when initializing it:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-4\", system_prompt=\"You are a useful assistant\")\n</code></pre> <p>This message will be used for every subsequent use of the model:</p>"},{"location":"reference/openai_text_generation/#usage","title":"Usage","text":""},{"location":"reference/openai_text_generation/#call-the-model","title":"Call the model","text":"<p>OpenAI models can be directly called with a prompt:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-3.5-turbo\")\nresult = model(\"Say something\", temperature=0, samples=2)\n</code></pre> <p>Warning</p> <p>This syntax will soon be deprecated and one will be able to generate text with OpenAI models with the same syntax used to generate text with Open Source models.</p>"},{"location":"reference/openai_text_generation/#stop-when-a-sequence-is-found","title":"Stop when a sequence is found","text":"<p>The OpenAI API tends to be chatty and it can be useful to stop the generation once a given sequence has been found, instead of paying for the extra tokens and needing to post-process the output. For instance if you only to generate a single sentence:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-4\")\nresponse = model(\"Write a sentence\", stop_at=['.'])\n</code></pre>"},{"location":"reference/openai_text_generation/#choose-between-multiple-choices","title":"Choose between multiple choices","text":"<p>It can be difficult to deal with a classification problem with the OpenAI API. However well you prompt the model, chances are you are going to have to post-process the output anyway. Sometimes the model will even make up choices. Outlines allows you to guarantee that the output of the model will be within a set of choices:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-3.5-turbo\")\nresult = model.generate_choice(\"Red or blue?\", [\"red\", \"blue\"])\n</code></pre> <p>Warning</p> <p>This syntax will soon be deprecated and one will be able to generate text with OpenAI models with the same syntax used to generate text with Open Source models.</p>"},{"location":"reference/openai_text_generation/#monitoring-api-use","title":"Monitoring API use","text":"<p>It is important to be able to track your API usage when working with OpenAI's API. The number of prompt tokens and completion tokens is directly accessible via the model instance:</p> <pre><code>import outlines.models\n\nmodel = models.openai(\"gpt-4\")\n\nprint(model.prompt_tokens)\n# 0\n\nprint(model.completion_tokens)\n# 0\n</code></pre> <p>These numbers are updated every time you call the model.</p>"},{"location":"reference/openai_text_generation/#vectorized-calls","title":"Vectorized calls","text":"<p>A unique feature of Outlines is that calls to the OpenAI API are vectorized (In the NumPy sense of the word). In plain English this means that you can call an Openai model with an array of prompts with arbitrary shape to an OpenAI model and it will return an array of answers. All calls are executed concurrently, which means this takes roughly the same time as calling the model with a single prompt:</p> <pre><code>from outlines import models\nfrom outlines import text\n\n@text.prompt\ndef template(input_numbers):\n    \"\"\"Use these numbers and basic arithmetic to get 24 as a result:\n\n    Input: {{ input_numbers }}\n    Steps: \"\"\"\n\nprompts = [\n    template([1, 2, 3]),\n    template([5, 9, 7]),\n    template([10, 12])\n]\n\nmodel = models.openai(\"text-davinci-003\")\nresults = model(prompts)\nprint(results.shape)\n# (3,)\n\nprint(type(results))\n# &lt;class 'numpy.ndarray'&gt;\n\nprint(results)\n# [\n#     \"\\n1. 1 + 2 x 3 = 7\\n2. 7 + 3 x 4 = 19\\n3. 19 + 5 = 24\",\n#     \"\\n1. Add the three numbers together: 5 + 9 + 7 = 21\\n2. Subtract 21 from 24: 24 - 21 = 3\\n3. Multiply the remaining number by itself: 3 x 3 = 9\\n4. Add the number with the multiplication result: 21 + 9 = 24\",\n#    \"\\n\\n1. Add the two numbers together: 10 + 12 = 22 \\n2. Subtract one of the numbers: 22 - 10 = 12 \\n3. Multiply the two numbers together: 12 x 12 = 144 \\n4. Divide the first number by the result: 144 / 10 = 14.4 \\n5. Add the initial two numbers together again: 14.4 + 12 = 26.4 \\n6. Subtract 2: 26.4 - 2 = 24\",\n# ]\n</code></pre> <p>Beware that in this case the output of the model is a NumPy array. So if you want to concatenate the prompt to the result you have to use <code>numpy.char.add</code>:</p> <pre><code>import numpy as np\n\nnew_prompts = np.char.add(prompts, results)\nprint(new_prompts)\n\n# [\n#     \"Use these numbers and basic arithmetic to get 24 as a result:\\n\\nInput: [1, 2, 3]\\nSteps:\\n1. 1 + 2 x 3 = 7\\n2. 7 + 3 x 4 = 19\\n3. 19 + 5 = 24\",\n#     \"Use these numbers and basic arithmetic to get 24 as a result:\\n\\nInput: [5, 9, 7]\\nSteps:\\n1. Add the three numbers together: 5 + 9 + 7 = 21\\n2. Subtract 21 from 24: 24 - 21 = 3\\n3. Multiply the remaining number by itself: 3 x 3 = 9\\n4. Add the number with the multiplication result: 21 + 9 = 24\",\n#    \"'Use these numbers and basic arithmetic to get 24 as a result:\\n\\nInput: [10, 12]\\nSteps:\\n\\n1. Add the two numbers together: 10 + 12 = 22 \\n2. Subtract one of the numbers: 22 - 10 = 12 \\n3. Multiply the two numbers together: 12 x 12 = 144 \\n4. Divide the first number by the result: 144 / 10 = 14.4 \\n5. Add the initial two numbers together again: 14.4 + 12 = 26.4 \\n6. Subtract 2: 26.4 - 2 = 24\",\n# ]\n</code></pre> <p>You can also ask for several samples for a single prompt:</p> <pre><code>from outlines import models\nfrom outlines import text\n\n\n@text.prompt\ndef template(input_numbers):\n    \"\"\"Use these numbers and basic arithmetic to get 24 as a result:\n\n    Input: {{ input_numbers }}\n    Steps:\"\"\"\n\n\nmodel = models.openai(\"text-davinci-003\")\nresults = model(template([1, 2, 3]), samples=3, stop_at=[\"\\n2\"])\nprint(results.shape)\n# (3,)\n\nprint(results)\n# [\n#     ' \\n1. Subtract 1 from 3',\n#     '\\n1. Add the three numbers: 1 + 2 + 3 = 6',\n#     ' (1 + 3) x (2 + 2) = 24'\n# ]\n</code></pre> <p>Or ask for several samples for an array of prompts. In this case the last dimension is the sample dimension:</p> <pre><code>from outlines import models\nfrom outlines import text\n\n\n@text.prompt\ndef template(input_numbers):\n    \"\"\"Use these numbers and basic arithmetic to get 24 as a result:\n\n    Input: {{ input_numbers }}\n    Steps:\"\"\"\n\n\nprompts = [template([1, 2, 3]), template([5, 9, 7]), template([10, 12])]\n\nmodel = models.openai(\"text-davinci-003\")\nresults = model(prompts, samples=2, stop_at=[\"\\n2\"])\nprint(results.shape)\n# (3, 2)\n\nprint(results)\n# [\n#     ['\\n1. Add the numbers: 1 + 2 + 3 = 6', ' (3 * 2) - 1 = 5\\n        5 * 4 = 20\\n        20 + 4 = 24'],\n#     ['\\n\\n1. (5 + 9) x 7 =  56', '\\n1. 5 x 9 = 45'],\n#     [' \\n1. Add the two numbers together: 10 + 12 = 22', '\\n1. Add 10 + 12']\n# ]\n</code></pre> <p>You may find this useful, e.g., to implement Tree of Thoughts.</p> <p>Note</p> <p>Outlines provides an <code>@outlines.vectorize</code> decorator that you can use on any <code>async</code> python function. This can be useful for instance when you call a remote API within your workflow.</p>"},{"location":"reference/openai_text_generation/#advanced-usage","title":"Advanced usage","text":"<p>It is possible to specify the values for <code>seed</code>, <code>presence_penalty</code>, <code>frequence_penalty</code>, <code>top_p</code> by passing an instance of <code>OpenAIConfig</code> when initializing the model:</p> <pre><code>from outlines.models.openai import OpenAIConfig\nfrom outlines import models\n\nconfig = OpenAIConfig(\n    presence_penalty=1.,\n    frequence_penalty=1.,\n    top_p=.95,\n    seed=0,\n)\nmodel = models.openai(\"gpt-4\", config=config)\n</code></pre>"},{"location":"reference/prompting/","title":"Prompting techniques","text":""},{"location":"reference/prompting/#prompt-templating","title":"Prompt templating","text":"<p>Outlines provides a powerful domain-specific language to write and manage prompts, via what we call prompt functions. Prompt functions are Python functions that contain a template for the prompt in their docstring, and their arguments correspond to the variables used in the prompt. When called, a prompt function returns the template rendered with the values of the arguments:</p> <pre><code>import outlines.text as text\n\n@text.prompt\ndef greetings(name, question):\n    \"\"\"Hello, {{ name }}!\n    {{ question }}\n    \"\"\"\n\nprompt = greetings(\"user\", \"How are you?\")\n# Hello, user!\n# How are you?\n</code></pre> <p>Outlines uses the Jinja templating engine to render prompts, which allows to easily compose complex prompts. No need for extra abstractions to write a prompt with few-shot examples, Jinja can handle that:</p> CodeOutput <pre><code>import outlines.text as text\n\n@text.prompt\ndef few_shots(instructions, examples, question):\n    \"\"\"\"{{ instructions }}\n\n    {% for examples in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n    {% endfor %}\n    Q: {{ question }}\n    \"\"\"\n\nprompt = few_shots(question, examples, question)\n</code></pre> <p>Something</p> <p>Please refer to the <code>Jinja documentation &lt;https://jinja.palletsprojects.com/en/3.1.x/&gt;</code>_ for more information about the syntax of the templating language. The Jinja syntax is powerful, and we recommend you take some time to read their documentation if building your prompts requires complex logic involving for instance loops and conditionals.</p>"},{"location":"reference/prompting/#tools","title":"Tools","text":"<p>Several projects (e.g.<code>Toolformer &lt;https://arxiv.org/abs/2302.04761&gt;</code>, <code>ViperGPT &lt;https://viper.cs.columbia.edu/&gt;</code>, <code>AutoGPT &lt;https://github.com/Significant-Gravitas/Auto-GPT&gt;</code>_, etc.) have shown that we can \"teach\" language models to use external functions by describing what these functions do in the prompt. In these projects the same information is often repeated twice: the function implementation, name, docstring, or arguments are copy-pasted in the prompt. This is cumbersome and error prone; you can directly pull this information from within an Outlines prompt function:</p> CodeOutput <pre><code>import outlines.text as text\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n\n@text.prompt\ndef tool_prompt(question, tool):\n    \"\"\"{{ question }}\n\n    COMMANDS\n    1. {{ tool | name }}: {{ tool | description }}, args: {{ tool | args }}\n\n    {{ tool | source }}\n    \"\"\"\n\ntool_prompt(\"Can you do something?\", my_tool)\n# Can you do something?\n#\n# COMMANDS\n# 1. my_tool: Tool description, args: arg1:str, arg2:int\n#\n# def my_tool(arg1: str, arg2: int):\n#     \"\"\"Tool description.\n#\n#     The rest of the docstring\n#     \"\"\"\n#     pass\n</code></pre> <p>Something</p>"},{"location":"reference/prompting/#json-response-format","title":"JSON response format","text":"<p>To build reliable chains with language models we often need to instruct them the format in which we would like them to return their response. Again the information is often repeated twice between creating the parsing function, and writing the desired schema in the prompt. You can directly pull the JSON schema of a pydantic model, or pretty print a dictionary from within an Outlines prompt function</p> Code <pre><code>from pydantic import BaseModel, Field\n\nimport outlines.text as text\n\nclass MyResponse(BaseModel):\n    field1: int = Field(description=\"an int\")\n    field2: str\n\n@text.prompt\ndef my_prompt(response_model):\n    \"\"\"{{ response_model | schema }}\"\"\"\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"an int\",\n#   \"field2\": \"&lt;field2&gt;\"\n# }\n</code></pre> Output <pre><code>response = {\n    \"field1\": \"&lt;field1&gt;\",\n    \"field2\": \"a string\"\n}\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"&lt;field1&gt;\",\n#   \"field2\": \"a string\"\n# }\n</code></pre>"},{"location":"reference/regex/","title":"Regular expressions","text":""},{"location":"reference/types/","title":"Type constraints","text":"<p>We can ask completions to be restricted to valid integers or floating-point numbers using the <code>type</code> keyword argument, respectively with the \u201cint\u201d or \u201cfloat\u201d value:</p> <pre><code>import outlines.models as models\n\ncomplete = models.text_completion.openai(\"text-davinci-002\")\nanswer = complete(\n    \"When I was 6 my sister was half my age. Now I\u2019m 70 how old is my sister?\",\n    type=\"int\"\n)\n</code></pre>"}]}